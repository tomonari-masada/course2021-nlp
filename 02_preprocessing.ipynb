{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "02_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIVXAmw_tMRv"
      },
      "source": [
        "# **テキストデータの扱い方：基本中の基本編**\n",
        "\n",
        "* テキストデータは、長い長い文字列。\n",
        "* 長い長い文字列のままでは、普通は分析できない。\n",
        "* 今回は、自然言語処理における基本的な前処理について学ぶ。\n",
        "* 今回は、英語データのみを扱う。\n",
        " * 日本語データは、次回。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXE1NNPpXbjI"
      },
      "source": [
        "今回のnotebook作成にあたって、下記のリポジトリを参考にしました。\n",
        "\n",
        " * https://github.com/dipanjanS/nlp_essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeSwPsGJFWj"
      },
      "source": [
        "## 1. str型のメソッドによる前処理\n",
        "\n",
        "* 例えば、大文字と小文字の間の変換などが実行できる。\n",
        "\n",
        " * 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp382lJJFWp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "572fab91-4123-4d04-bce2-6de7f5a43fea"
      },
      "source": [
        "text = 'The quick brown fox jumped over The Big Dog'\n",
        "text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The quick brown fox jumped over The Big Dog'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaAwb7HZJFWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6c38499-574c-4d59-9760-daa77d50cfe5"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the quick brown fox jumped over the big dog'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihX9LwVuJFW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "337954b8-fcef-480d-e468-b4aa393b578c"
      },
      "source": [
        "text.upper()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19LzVfl7Tcje"
      },
      "source": [
        "* 各トークンの一文字目を大文字にする。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U24TBZ82JFW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d104356-ca24-455c-f191-3d8c58830fe7"
      },
      "source": [
        "text.title()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Quick Brown Fox Jumped Over The Big Dog'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCf5TIaJpEr"
      },
      "source": [
        "## 2. NLTK\n",
        "\n",
        "* NLTKは、Pythonの有名な自然言語処理ライブラリ。2001年スタートらしい。\n",
        "\n",
        "* https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSG71XiJoka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb50001-62e7-4937-c026-c6f363fadf55"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GzHq46JFW_"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "* 文に分ける、単語に分ける、など、長い文字列としての言語データをより小さな単位へと分割することを、一般にtokenizationと言う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFm7LXKmnoS9"
      },
      "source": [
        "* Pythonの文字列は、複数行にわたっていても、丸括弧でくくれば一つの長い文字列になる。\n",
        " * ただし、最後の行を除いて、末尾に空白を入れておくのを忘れないように。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIiPr5JBJFXA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a5d224ed-f606-47e0-9ccd-85dd22b4ed1c"
      },
      "source": [
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "sample_text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqUvs2QjoAy-"
      },
      "source": [
        "* 文ごとにtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2m8nEPmJFXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7911a9e7-580e-47dc-f1fd-1877446e0a57"
      },
      "source": [
        "nltk.sent_tokenize(sample_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqCjdGno8FZ7"
      },
      "source": [
        "* 問：下に示すword tokenizationのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDN_9m_oD7E"
      },
      "source": [
        "* 単語ごとにtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVNIwLoJFXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba9388f-d26b-4dba-f313-51b1750a99d0"
      },
      "source": [
        "print(nltk.word_tokenize(sample_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25190-Ft9Ls"
      },
      "source": [
        "## 3. spaCyを使ってみる\n",
        "\n",
        "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
        "\n",
        "* https://spacy.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5twUUnuIPf"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhORAuPJFXL"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6LA_YHJFXN"
      },
      "source": [
        "text_spacy = nlp(sample_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_nN_l3zQwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef67a2a-0408-492e-827b-74abc11e9fff"
      },
      "source": [
        "[obj.text for obj in text_spacy.sents]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR2DcOE18LfI"
      },
      "source": [
        "* 問： 下のword tokenizationは、先ほどのword tokenizationとどう違うか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBuAHdR8JFXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daba7171-fa77-4c80-92a9-c0cc68d11c7d"
      },
      "source": [
        "print([obj.text for obj in text_spacy])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhxnJkIsJFXS"
      },
      "source": [
        "## 4. HTML文書の前処理\n",
        "\n",
        "* __`<p>`__や__`<a>`__や__`<div>`__など、頻繁に使うHTMLタグは頭に入れておいてください。\n",
        "\n",
        "* なぜなら、ある程度HTMLタグが読めてはじめて、スクレイピングのコードを書くための、HTMLソースの下調べができるからです。\n",
        " * 自前でWeb上から分析対象のテキストデータを取得するときは、ダウンロードしようとするWebページのHTMLの構造を自分の目で確認する。\n",
        "\n",
        "* 問：誰かによって整備されたデータセットではなく、自前でHTML文書をスクレイピングすることのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewyVIu3auUl"
      },
      "source": [
        "### HTML文書のダウンロード\n",
        "* いくつか方法はあるが、ここではrequestsモジュールを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qV1WOpJFXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf597c0-033b-4527-c44a-d3d6ca30bb81"
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[2745:3948])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%;\t\t\t/* adjust to ape original work */\r\n",
            "\t\tmargin-top: 1em;\t/* space above &amp; below */\r\n",
            "\t\tmargin-bottom: 1em;\r\n",
            "\t\tmargin-left: auto;  /* these two ensure a.. */\r\n",
            "\t\tmargin-right: auto; /* ..centered rule */\r\n",
            "\t\tclear: both;\t\t/* don't let sidebars &amp; floats overlap rule */\r\n",
            "\t}\r\n",
            "/* ************************************************************************\r\n",
            " * Images and captions\r\n",
            " * ********************************************************************** */\r\n",
            "\timg { /* the default inline image has */\r\n",
            "\t\tborder: 1px solid black; /* a thin black line border.. */\r\n",
            "\t\tpadding: 6px; /* ..spaced a bit out from the graphic */\r\n",
            "\t\t} </style>\r\n",
            "<link rel=\"schema.dc\" href=\"http://purl.org/dc/elements/1.1/\">\r\n",
            "<link rel=\"schema.dcterms\" href=\"http://purl.org/dc/terms/\">\r\n",
            "<meta name=\"dc.title\" content=\"The Bible, King James version, Book 1: Genesis\">\r\n",
            "<meta name=\"dc.language\" content=\"en\">\r\n",
            "<meta name=\"dcterms.source\" content=\"https://www.gutenberg.org/files/8001/8001.txt\">\r\n",
            "<meta name=\"dcterms.modified\" content=\"2021-09-04T09:42:22.231886+00:00\">\r\n",
            "<meta name=\"dc.rights\" content=\"Public domain in the USA.\">\r\n",
            "<link rel=\"dcterms.isFormatOf\" href=\"http://www.gutenberg.org/ebooks/8001\">\r\n",
            "<meta name=\"dc.creato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vte11lpuXV_"
      },
      "source": [
        "### Beautiful Soupの利用\n",
        "\n",
        "* HTML文書の構造を解析するためによく使われるライブラリ。\n",
        "\n",
        "* 参考資料：「Beautiful Soup 4によるスクレイピングの基礎」\n",
        "\n",
        " * https://www.atmarkit.co.jp/ait/articles/1910/18/news015.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UAz3mjJFXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fa9c69-5c28-48f3-e367-3ce85c8bcfa9"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    # 下の正規表現の意味を説明してみよう。\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:1957])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llowing heads  */\n",
            "\th2+p, h3+p, h4+p { text-indent: 0; }\n",
            "\t/* tighter spacing for list item paragraphs */\n",
            "\tdd, li {\n",
            "\t\tmargin-top: 0.25em; margin-bottom:0;\n",
            "\t\tline-height: 1.2em; /* a bit closer than p's */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Head 2 is for chapter heads. \n",
            " * ********************************************************************** */\n",
            "\th2 {\n",
            "\t\t/* text-align:center;  left-aligned by default. */\n",
            "\t\tmargin-top:3em;\t\t/* extra space above.. */\n",
            "\t\tmargin-bottom: 2em;\t/* ..and below */\n",
            "\t\tclear: both;\t\t/* don't let sidebars overlap */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Head 3 is for main-topic heads.\n",
            " * ********************************************************************** */\n",
            "\th3 {\n",
            "\t\t\t/* text-a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au1nuTllwX07"
      },
      "source": [
        "## 演習1\n",
        "* clean_contentを単語に分割し、各単語の出現頻度を求め、出現頻度の高い順に上位100の単語を、出現頻度とともに表示しよう。\n",
        "* clean_contentの内容をすべて小文字に変換した後で同じことをしてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdGJ9gYO_j1o"
      },
      "source": [
        "# 演習1-1の答案\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwDPU2fx0i0"
      },
      "source": [
        "## 5. 様々な前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fJi5YyKJFXc"
      },
      "source": [
        "### アクセント記号の除去\n",
        "\n",
        "* unicodedataというライブラリを使う。\n",
        "\n",
        " * https://docs.python.org/3/library/unicodedata.html\n",
        "\n",
        "* 'NFKD'は何を意味するか？ （Wikipedia「Unicode正規化」）\n",
        "\n",
        " * https://ja.wikipedia.org/wiki/Unicode%E6%AD%A3%E8%A6%8F%E5%8C%96\n",
        "\n",
        "* PythonにおけるUnicode HOWTO\n",
        "\n",
        " * https://docs.python.org/ja/3/howto/unicode.html\n",
        "\n",
        "* テキストデータの前処理においてアクセント記号を除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps9wmhv9JFXd"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc7JR8CQJFXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0347e5e6-f0cd-4489-9e02-92aca9e8db7d"
      },
      "source": [
        "s = (\"Le bon sens est la chose du monde la mieux partagée; car chacun pense \"\n",
        "  \"en être si bien pourvu, que ceux même qui sont les plus difficiles à \"\n",
        "  \"contenter en toute autre chose n'ont point coutume d'en désirer plus \"\n",
        "  \"qu'ils en ont.\")\n",
        "s"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Le bon sens est la chose du monde la mieux partagée; car chacun pense en être si bien pourvu, que ceux même qui sont les plus difficiles à contenter en toute autre chose n'ont point coutume d'en désirer plus qu'ils en ont.\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6a-e-mVJFXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08dbd864-980e-444f-a733-ebe9d32291a1"
      },
      "source": [
        "remove_accented_chars(s)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Le bon sens est la chose du monde la mieux partagee; car chacun pense en etre si bien pourvu, que ceux meme qui sont les plus difficiles a contenter en toute autre chose n'ont point coutume d'en desirer plus qu'ils en ont.\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8CyGmPJFXr"
      },
      "source": [
        "### 特殊文字、数字、記号の除去\n",
        "\n",
        "* reモジュールを使う。reはregular expression(正規表現)のこと。\n",
        "\n",
        "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wThZGxr4oTFT"
      },
      "source": [
        "* 問：下のセルで使われている２つの正規表現はそれぞれどういう意味か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dkc4ESDJFXs"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUwKvQ-1JFXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2dfe4ca-8afb-459a-fbc3-9d8105a3d6e5"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9x4XFyJFYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11894d19-d48e-4fa0-fd26-90b87a8778c7"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun See you at  What do you think  '"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2vT0GK5JFYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "595873fa-eec2-4809-bfbe-71fc6468ccf4"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun See you at 730 What do you think 9318 '"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6h68QbJFYX"
      },
      "source": [
        "### Contraction\n",
        "\n",
        "* 英語には様々な省略表現がある。これを元に戻す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgGTT1URJFYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93c17b1-f2ad-47c7-bf2a-929879d688e7"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 43.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85439 sha256=081b1551033d689682372967ef7f92ed424a93735659ab10adc00f77a62b3e61\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWsgO-jJFYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24152a53-99b1-44d9-fc03-c7121abbf930"
      },
      "source": [
        "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
        "s"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2QTF2HFJFYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1107a818-14ab-4f0a-9a9d-d2ac558aaee0"
      },
      "source": [
        "import contractions\n",
        "\n",
        "contractions_list = list(contractions.contractions_dict.items())\n",
        "print(len(contractions_list))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm2e0y-Vymmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a172c4-2ac5-4cf2-c1f2-d3800c5d776e"
      },
      "source": [
        "print(contractions_list)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"I'm\", 'I am'), (\"I'm'a\", 'I am about to'), (\"I'm'o\", 'I am going to'), (\"I've\", 'I have'), (\"I'll\", 'I will'), (\"I'll've\", 'I will have'), (\"I'd\", 'I would'), (\"I'd've\", 'I would have'), ('Whatcha', 'What are you'), (\"amn't\", 'am not'), (\"ain't\", 'are not'), (\"aren't\", 'are not'), (\"'cause\", 'because'), (\"can't\", 'cannot'), (\"can't've\", 'cannot have'), (\"could've\", 'could have'), (\"couldn't\", 'could not'), (\"couldn't've\", 'could not have'), (\"daren't\", 'dare not'), (\"daresn't\", 'dare not'), (\"dasn't\", 'dare not'), (\"didn't\", 'did not'), ('didn’t', 'did not'), (\"don't\", 'do not'), ('don’t', 'do not'), (\"doesn't\", 'does not'), (\"e'er\", 'ever'), (\"everyone's\", 'everyone is'), ('finna', 'fixing to'), ('gimme', 'give me'), (\"gon't\", 'go not'), ('gonna', 'going to'), ('gotta', 'got to'), (\"hadn't\", 'had not'), (\"hadn't've\", 'had not have'), (\"hasn't\", 'has not'), (\"haven't\", 'have not'), (\"he've\", 'he have'), (\"he's\", 'he is'), (\"he'll\", 'he will'), (\"he'll've\", 'he will have'), (\"he'd\", 'he would'), (\"he'd've\", 'he would have'), (\"here's\", 'here is'), (\"how're\", 'how are'), (\"how'd\", 'how did'), (\"how'd'y\", 'how do you'), (\"how's\", 'how is'), (\"how'll\", 'how will'), (\"isn't\", 'is not'), (\"it's\", 'it is'), (\"'tis\", 'it is'), (\"'twas\", 'it was'), (\"it'll\", 'it will'), (\"it'll've\", 'it will have'), (\"it'd\", 'it would'), (\"it'd've\", 'it would have'), ('kinda', 'kind of'), (\"let's\", 'let us'), ('luv', 'love'), (\"ma'am\", 'madam'), (\"may've\", 'may have'), (\"mayn't\", 'may not'), (\"might've\", 'might have'), (\"mightn't\", 'might not'), (\"mightn't've\", 'might not have'), (\"must've\", 'must have'), (\"mustn't\", 'must not'), (\"mustn't've\", 'must not have'), (\"needn't\", 'need not'), (\"needn't've\", 'need not have'), (\"ne'er\", 'never'), (\"o'\", 'of'), (\"o'clock\", 'of the clock'), (\"ol'\", 'old'), (\"oughtn't\", 'ought not'), (\"oughtn't've\", 'ought not have'), (\"o'er\", 'over'), (\"shan't\", 'shall not'), (\"sha'n't\", 'shall not'), (\"shalln't\", 'shall not'), (\"shan't've\", 'shall not have'), (\"she's\", 'she is'), (\"she'll\", 'she will'), (\"she'd\", 'she would'), (\"she'd've\", 'she would have'), (\"should've\", 'should have'), (\"shouldn't\", 'should not'), (\"shouldn't've\", 'should not have'), (\"so've\", 'so have'), (\"so's\", 'so is'), (\"somebody's\", 'somebody is'), (\"someone's\", 'someone is'), (\"something's\", 'something is'), ('sux', 'sucks'), (\"that're\", 'that are'), (\"that's\", 'that is'), (\"that'll\", 'that will'), (\"that'd\", 'that would'), (\"that'd've\", 'that would have'), ('em', 'them'), (\"there're\", 'there are'), (\"there's\", 'there is'), (\"there'll\", 'there will'), (\"there'd\", 'there would'), (\"there'd've\", 'there would have'), (\"these're\", 'these are'), (\"they're\", 'they are'), (\"they've\", 'they have'), (\"they'll\", 'they will'), (\"they'll've\", 'they will have'), (\"they'd\", 'they would'), (\"they'd've\", 'they would have'), (\"this's\", 'this is'), (\"those're\", 'those are'), (\"to've\", 'to have'), ('wanna', 'want to'), (\"wasn't\", 'was not'), (\"we're\", 'we are'), (\"we've\", 'we have'), (\"we'll\", 'we will'), (\"we'll've\", 'we will have'), (\"we'd\", 'we would'), (\"we'd've\", 'we would have'), (\"weren't\", 'were not'), (\"what're\", 'what are'), (\"what'd\", 'what did'), (\"what've\", 'what have'), (\"what's\", 'what is'), (\"what'll\", 'what will'), (\"what'll've\", 'what will have'), (\"when've\", 'when have'), (\"when's\", 'when is'), (\"where're\", 'where are'), (\"where'd\", 'where did'), (\"where've\", 'where have'), (\"where's\", 'where is'), (\"which's\", 'which is'), (\"who're\", 'who are'), (\"who've\", 'who have'), (\"who's\", 'who is'), (\"who'll\", 'who will'), (\"who'll've\", 'who will have'), (\"who'd\", 'who would'), (\"who'd've\", 'who would have'), (\"why're\", 'why are'), (\"why'd\", 'why did'), (\"why've\", 'why have'), (\"why's\", 'why is'), (\"will've\", 'will have'), (\"won't\", 'will not'), (\"won't've\", 'will not have'), (\"would've\", 'would have'), (\"wouldn't\", 'would not'), (\"wouldn't've\", 'would not have'), (\"y'all\", 'you all'), (\"y'all're\", 'you all are'), (\"y'all've\", 'you all have'), (\"y'all'd\", 'you all would'), (\"y'all'd've\", 'you all would have'), (\"you're\", 'you are'), (\"you've\", 'you have'), (\"you'll've\", 'you shall have'), (\"you'll\", 'you will'), (\"you'd\", 'you would'), (\"you'd've\", 'you would have'), ('jan.', 'january'), ('feb.', 'february'), ('mar.', 'march'), ('apr.', 'april'), ('jun.', 'june'), ('jul.', 'july'), ('aug.', 'august'), ('sep.', 'september'), ('oct.', 'october'), ('nov.', 'november'), ('dec.', 'december'), ('I’m', 'I am'), ('I’m’a', 'I am about to'), ('I’m’o', 'I am going to'), ('I’ve', 'I have'), ('I’ll', 'I will'), ('I’ll’ve', 'I will have'), ('I’d', 'I would'), ('I’d’ve', 'I would have'), ('amn’t', 'am not'), ('ain’t', 'are not'), ('aren’t', 'are not'), ('’cause', 'because'), ('can’t', 'cannot'), ('can’t’ve', 'cannot have'), ('could’ve', 'could have'), ('couldn’t', 'could not'), ('couldn’t’ve', 'could not have'), ('daren’t', 'dare not'), ('daresn’t', 'dare not'), ('dasn’t', 'dare not'), ('doesn’t', 'does not'), ('e’er', 'ever'), ('everyone’s', 'everyone is'), ('gon’t', 'go not'), ('hadn’t', 'had not'), ('hadn’t’ve', 'had not have'), ('hasn’t', 'has not'), ('haven’t', 'have not'), ('he’ve', 'he have'), ('he’s', 'he is'), ('he’ll', 'he will'), ('he’ll’ve', 'he will have'), ('he’d', 'he would'), ('he’d’ve', 'he would have'), ('here’s', 'here is'), ('how’re', 'how are'), ('how’d', 'how did'), ('how’d’y', 'how do you'), ('how’s', 'how is'), ('how’ll', 'how will'), ('isn’t', 'is not'), ('it’s', 'it is'), ('’tis', 'it is'), ('’twas', 'it was'), ('it’ll', 'it will'), ('it’ll’ve', 'it will have'), ('it’d', 'it would'), ('it’d’ve', 'it would have'), ('let’s', 'let us'), ('ma’am', 'madam'), ('may’ve', 'may have'), ('mayn’t', 'may not'), ('might’ve', 'might have'), ('mightn’t', 'might not'), ('mightn’t’ve', 'might not have'), ('must’ve', 'must have'), ('mustn’t', 'must not'), ('mustn’t’ve', 'must not have'), ('needn’t', 'need not'), ('needn’t’ve', 'need not have'), ('ne’er', 'never'), ('o’', 'of'), ('o’clock', 'of the clock'), ('ol’', 'old'), ('oughtn’t', 'ought not'), ('oughtn’t’ve', 'ought not have'), ('o’er', 'over'), ('shan’t', 'shall not'), ('sha’n’t', 'shall not'), ('shalln’t', 'shall not'), ('shan’t’ve', 'shall not have'), ('she’s', 'she is'), ('she’ll', 'she will'), ('she’d', 'she would'), ('she’d’ve', 'she would have'), ('should’ve', 'should have'), ('shouldn’t', 'should not'), ('shouldn’t’ve', 'should not have'), ('so’ve', 'so have'), ('so’s', 'so is'), ('somebody’s', 'somebody is'), ('someone’s', 'someone is'), ('something’s', 'something is'), ('that’re', 'that are'), ('that’s', 'that is'), ('that’ll', 'that will'), ('that’d', 'that would'), ('that’d’ve', 'that would have'), ('there’re', 'there are'), ('there’s', 'there is'), ('there’ll', 'there will'), ('there’d', 'there would'), ('there’d’ve', 'there would have'), ('these’re', 'these are'), ('they’re', 'they are'), ('they’ve', 'they have'), ('they’ll', 'they will'), ('they’ll’ve', 'they will have'), ('they’d', 'they would'), ('they’d’ve', 'they would have'), ('this’s', 'this is'), ('those’re', 'those are'), ('to’ve', 'to have'), ('wasn’t', 'was not'), ('we’re', 'we are'), ('we’ve', 'we have'), ('we’ll', 'we will'), ('we’ll’ve', 'we will have'), ('we’d', 'we would'), ('we’d’ve', 'we would have'), ('weren’t', 'were not'), ('what’re', 'what are'), ('what’d', 'what did'), ('what’ve', 'what have'), ('what’s', 'what is'), ('what’ll', 'what will'), ('what’ll’ve', 'what will have'), ('when’ve', 'when have'), ('when’s', 'when is'), ('where’re', 'where are'), ('where’d', 'where did'), ('where’ve', 'where have'), ('where’s', 'where is'), ('which’s', 'which is'), ('who’re', 'who are'), ('who’ve', 'who have'), ('who’s', 'who is'), ('who’ll', 'who will'), ('who’ll’ve', 'who will have'), ('who’d', 'who would'), ('who’d’ve', 'who would have'), ('why’re', 'why are'), ('why’d', 'why did'), ('why’ve', 'why have'), ('why’s', 'why is'), ('will’ve', 'will have'), ('won’t', 'will not'), ('won’t’ve', 'will not have'), ('would’ve', 'would have'), ('wouldn’t', 'would not'), ('wouldn’t’ve', 'would not have'), ('y’all', 'you all'), ('y’all’re', 'you all are'), ('y’all’ve', 'you all have'), ('y’all’d', 'you all would'), ('y’all’d’ve', 'you all would have'), ('you’re', 'you are'), ('you’ve', 'you have'), ('you’ll’ve', 'you shall have'), ('you’ll', 'you will'), ('you’d', 'you would'), ('you’d’ve', 'you would have')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoIGJXqCJFYo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9085b733-1426-42c3-d39e-ebee2b30ef06"
      },
      "source": [
        "contractions.fix(s)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you all cannot expand contractions I would think! You would not be able to. how did you do it?'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC54E8pjzGMu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc48910c-c1e3-4291-c30d-f2e8fefaf0cd"
      },
      "source": [
        "s = \"It's pool-season from this week, isn't it? Oh yes. I've gotta go and buy a swimming suit, then.\"\n",
        "contractions.fix(s)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it is pool-season from this week, is not it? Oh yes. I have got to go and buy a swimming suit, then.'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeUHPmhDJFZC"
      },
      "source": [
        "### NLTKでstemming\n",
        "\n",
        "* 語尾が変化する単語の、その変化を無くして、語幹を得る。\n",
        "* 得られる語幹は、英単語として通用しない文字列になることが多い。\n",
        "* Stemming and Lemmatization in Python\n",
        " * https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
        "\n",
        "\n",
        "* 問：テキストデータの前処理としてstemmingをすることのメリットとデメリットは何か？\n",
        "* 問：様々な種類のstemmerがあるのはなぜか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKIclu60ob5A"
      },
      "source": [
        "* Porter Stemmerを使ってみる （stemmerと言えばこれ、というぐらい良く知られている。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ndJ4XOKJFZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00c3bff-999d-4092-94d3-9713c2905b7f"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmWLISH-JFZG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82dc6377-1205-4b51-9de2-f6fd0afa7775"
      },
      "source": [
        "ps.stem('lying')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lie'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7KRj1jtJFZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec50dda5-ae24-43cf-a37e-ba4b48684dd3"
      },
      "source": [
        "ps.stem('strange')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'strang'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQNUmpfLJFZu"
      },
      "source": [
        "### NLTKでlemmatization（不完全版）\n",
        "\n",
        "* 語形が変わる単語を原型に戻す。\n",
        "* 原型は、英語の単語として通用する。\n",
        "\n",
        "* 問：テキストデータの前処理としてlemmatizationをすることのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOcAGKy5ofah"
      },
      "source": [
        "* WordNetを辞書として使うlemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ygP7t1JFZv"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieUIjYaJFZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7545d9-a0de-4c9e-d704-4418bbf0ff2d"
      },
      "source": [
        "help(wnl.lemmatize)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method lemmatize in module nltk.stem.wordnet:\n",
            "\n",
            "lemmatize(word, pos='n') method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DrMWzAopxj"
      },
      "source": [
        "* 名詞のlemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZPcwz44JFZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6486c236-1401-434e-c5e7-1a77228aa63f"
      },
      "source": [
        "print(wnl.lemmatize('cars', 'n'))\n",
        "print(wnl.lemmatize('boxes', 'n'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n",
            "box\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4crzKsTXosJy"
      },
      "source": [
        "* 動詞のlemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJN-uQ28JFZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffb949d-e756-4bad-b85a-9d62a4500997"
      },
      "source": [
        "print(wnl.lemmatize('running', 'v'))\n",
        "print(wnl.lemmatize('ate', 'v'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNakwg9zoyOl"
      },
      "source": [
        "* 形容詞のlemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0u5uZeoJFaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b306fa-c68f-42b4-9646-d6a939b88db0"
      },
      "source": [
        "print(wnl.lemmatize('saddest', 'a'))\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sad\n",
            "fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExDXjT4uoiWE"
      },
      "source": [
        "* 指定した品詞が間違っていると、うまくいかない。\n",
        " * 品詞を取得する方法は、すぐ後で解説する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhKXkdckJFaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8633de22-9156-4fb2-dfd8-dc4a572a8bab"
      },
      "source": [
        "print(wnl.lemmatize('ate', 'n'))\n",
        "print(wnl.lemmatize('fancier', 'v'))\n",
        "print(wnl.lemmatize('fancier'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ate\n",
            "fancier\n",
            "fancier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ1S2ngz7B84"
      },
      "source": [
        "### NLTKによるtokenizationとlemmatizationの組み合わせ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l372SiEJFaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8868ebd5-7f0e-450b-8af6-730135b41886"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "\n",
        "tokens = nltk.word_tokenize(s)\n",
        "print(tokens)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSO2mxi_olcX"
      },
      "source": [
        "* ここでのlemmatizationは、品詞情報を使っていないので、不完全。下でこれを改良する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1FHAghFJFaX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bd5acc2-cf51-4e5b-b9fc-c5847ec68c29"
      },
      "source": [
        "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
        "lemmatized_text"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-fgmbi7E5_"
      },
      "source": [
        "### POS Tagging\n",
        "\n",
        "* 問：品詞の情報が必要になるのはどういうときか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDffFU3gJFaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6656be82-b64c-4b5d-cfd7-ceb11ddfe1d5"
      },
      "source": [
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "print(tagged_tokens)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9STnRHVt7HRG"
      },
      "source": [
        "* NLTKが与えるPOSタグをWordNetのPOSタグに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S9kS_xPJFaf"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def pos_tag_wordnet(tagged_tokens):\n",
        "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
        "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
        "                            for word, tag in tagged_tokens]\n",
        "    return new_tagged_tokens"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbijTK6YJFaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fa09e1-62f6-4867-8def-5e02a2b0ad90"
      },
      "source": [
        "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
        "print(wordnet_tokens)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKia_-ov7KLH"
      },
      "source": [
        "### NLTKでlemmatization（完全版）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNOpTLDTJFal",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48e181f3-84ee-4ccf-dc2d-af4cec20aebf"
      },
      "source": [
        "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "lemmatized_text"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zisZIs1JFan"
      },
      "source": [
        "## 演習2\n",
        "\n",
        "* 上の3つのセルでおこなった処理をまとめて一つの関数として定義しよう。\n",
        " - 関数 __`wordnet_lemmatize_text()`__ を定義する。\n",
        " - 入力は変数 __`text`__ とし、これは文字列とする。\n",
        " - この関数のなかで、さきほど定義した関数__`pos_tag_wordnet()`__を使う。\n",
        " - そして、lemmatizeされたテキストを文字列型の出力として返すようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPovRPZb5PVG"
      },
      "source": [
        "# 演習1-2の答案\n",
        "# def wordnet_lemmatize_text(text):\n",
        "#   ..........\n",
        "\n",
        "\n",
        "\n",
        "#s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "#wordnet_lemmatize_text(s)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgQJp2SH7OC_"
      },
      "source": [
        "### spaCyでlemmatization\n",
        "\n",
        "* 上のように、別途品詞を調べる必要はない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2ExlFqJFaw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
        "\n",
        "def spacy_lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-E47JKJFaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c195eb3-d517-4267-b5a0-34262b09d9ea"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "s"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-PrIeqJFa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7b84e76-5818-4cdb-a6f6-debc6434a507"
      },
      "source": [
        "spacy_lemmatize_text(s)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsKAXlvJFa7"
      },
      "source": [
        "## 6. ストップワード\n",
        "\n",
        "* ストップワードとは、言語データを分析するにあたって、非常に頻繁に使われるため内容の分析にあまり役に立たない単語のことを言う。\n",
        "\n",
        "* これこそが英語のストップワードだ！と言えるような決定的なストップワードのリストがあるわけではない。\n",
        "\n",
        " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
        "\n",
        " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaYuRW2AxvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df15547b-1506-47d4-d798-4bc247b88aa4"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'own', 'the', '‘s', 'below', 'namely', 'mine', 'something', 'both', 'they', 'might', 'upon', 'through', 'even', 'latter', 'as', 'least', 'of', 'within', 'else', \"'m\", 'whence', 'whole', 'forty', 'how', 'eight', 'beforehand', 'became', 'two', 'thence', 'sixty', 'n‘t', 'side', 'regarding', 'before', 'onto', 'again', 'anywhere', 'each', 'see', 'what', 'back', 'many', 'can', 'everything', 'be', 'behind', 'somehow', 'nor', 'seemed', 'next', 'really', 'either', 'once', 'at', 'towards', 'ourselves', 'is', 'someone', '’d', 'for', 'latterly', 'along', 'hers', 'so', 'more', 'thereupon', 'have', 'nobody', 'into', 'rather', 'besides', 'she', 'some', 'elsewhere', 'six', 'thereby', '‘ve', 'had', 'itself', 'well', 'whom', 'whereafter', 'often', 'our', 'herself', 'a', 'however', 'further', 'are', 'that', 'an', 'alone', \"n't\", 'most', 'anyone', 'last', 'fifty', 'various', 'though', 'but', 'themselves', 'via', 'its', 'among', 'off', 'my', 'three', 'you', 'me', 'almost', 'being', 'here', 'between', 'has', 'or', 'since', 'if', 'out', 'whereas', 'who', 'yourself', 'hereupon', 'nowhere', 'nevertheless', 'go', 'every', 'these', 'everyone', 'without', \"'re\", 'your', 'under', 'name', 'may', 'already', 'meanwhile', '‘d', 'used', 'still', 'around', 'becoming', 'their', \"'ve\", 'whose', 'per', 'formerly', 'former', 'doing', 'third', 'anyway', 'now', 'which', 'anything', 'another', 'therein', 'those', 'throughout', 'move', 'put', 'also', 'bottom', '‘m', 'ten', 'whenever', 'mostly', 'get', 'seeming', 'this', 'when', 'hence', 'them', 'until', 'together', 'other', 'on', 'no', 'while', 'everywhere', 'say', 'several', 'him', 'whereby', 'yours', 'from', 'whither', 'such', 'take', 'using', 'due', 'after', 'same', 'was', 'beyond', 'anyhow', 'wherever', 'with', 'n’t', '‘ll', 'thus', 'perhaps', 'am', 'indeed', 'very', 'not', 'unless', 'done', 'any', 'always', 'ours', 'toward', 'hereafter', 'enough', 'did', 'nine', 'please', 'ever', 'become', 'her', 'fifteen', 'during', 'hereby', 'thereafter', 'up', 'five', 'four', '’ve', 'less', 'serious', \"'d\", 'others', 'thru', 'yourselves', 'although', 'seems', 'somewhere', 'above', 'we', 'one', 'were', 'by', 'sometime', 'cannot', \"'ll\", 'in', 'make', 'whatever', 'becomes', 'there', 'keep', 'therefore', 'been', 'because', 'must', 'none', 'only', 'his', 'quite', 'where', 'whereupon', 'front', 'except', 'seem', 'he', 'call', 'would', 'down', 'never', 'then', 'top', 'does', 'eleven', 'nothing', 'sometimes', 'twenty', 'wherein', 'us', 'just', 'myself', 'amount', 'twelve', 'than', 'show', 'yet', 'across', \"'s\", '’m', 'made', 'against', '’s', 'all', 'afterwards', 'moreover', 'part', 'amongst', 'beside', 'herein', '’re', 'do', 'neither', 'few', 'why', 'to', 'give', 'otherwise', 'about', 'himself', 'should', 'and', 'first', 'it', 'hundred', 're', 'over', 'too', 'will', 'full', '‘re', 'ca', 'noone', 'could', 'whoever', 'empty', 'much', 'i', '’ll', 'whether'}\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJLKKxrJFa7"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def remove_stopwords(text, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "    tokens = [obj.text for obj in nlp(text)]\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycusSsPBJFbA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b4c3a2b-afa8-4745-cd2b-e17db00bf731"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "s"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWKjTPnzJFbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "440f98d4-69cf-4401-8f47-a71665173e44"
      },
      "source": [
        "remove_stopwords(s)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes quick jumping sleeping lazy dogs !'"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfSpyrrCJ63"
      },
      "source": [
        "## 7. 様々なSegmentation\n",
        "* 図表は下記のブログ記事より。\n",
        " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
        "\n",
        "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/Segmentation.png)\n",
        "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/inherent_task_complexity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj8RJ6EoCwgp"
      },
      "source": [
        "# 課題2\n",
        "\n",
        "* Wikipediaの適当な英語のエントリをダウンロードする。\n",
        "\n",
        " * 選ぶのが面倒という方はAIのエントリでもどうぞ。\n",
        "   * https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "\n",
        "* BeautifulSoupで本文のテキストだけを取得する。\n",
        "\n",
        " * HTMLのソースを見て、どこが本文かを確認する。\n",
        " * あるいは、ネット検索をして、Wikipediaのエントリから本文だけを取得する方法を調べる。\n",
        "\n",
        "* 以下の前処理をする。\n",
        "\n",
        " * 大文字は小文字にする。ただし固有名詞を除く。\n",
        "\n",
        " * ストップワードを除去する。\n",
        "\n",
        " * lemmatizationする。\n",
        "\n",
        "* 各単語の出現回数を求め、表示する。\n",
        "\n",
        "* lemmatizationした後の単語を、元の出現順序どおりに、半角スペースで区切ってつなぎ、長い一つの文字列にする。\n",
        " * joinメソッドを使えばよい。つまり、__`' '.join(`__ lemmatizeされた単語のリスト __`)`__ という感じ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDMb9o6cgMg"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    }
  ]
}