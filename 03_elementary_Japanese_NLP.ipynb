{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_elementary_Japanese_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaEItdNGpP4RFuojrQThXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/03_elementary_Japanese_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_GmRdAi6h89"
      },
      "source": [
        "# **日本語データの扱い方**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4X6Ihrby4V5"
      },
      "source": [
        "* 日本語は、言語の性質上、文が空白によって単語へ分たれていない。\n",
        " * 同じように、文が空白によって単語へ分たれていない言語として、他にどのような言語があるか？\n",
        "* そのため、まず最初に文を単語へ分割する必要がある。\n",
        "* この作業を**形態素解析**と言う。\n",
        " * 長い文字列としての言語データを、単語のような細かい単位へ分割することを、一般にtokenizationと言う。\n",
        " * tokenizationの単位は、単語のように意味的なまとまりをもつ単位とは限らない。\n",
        " *cf. [SentencePiece](https://arxiv.org/pdf/1808.06226.pdf) ... [この解説](https://qiita.com/taku910/items/7e52f1e58d0ea6e7859c)も参照。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vALkrVWnUbq4"
      },
      "source": [
        "* ここでは、spaCyを介してSudachiという形態素解析器を使う。\n",
        " * 形態素解析器としては、他には、MeCabやJUMANが有名。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyUrPpHK-woI"
      },
      "source": [
        "* 注\n",
        " * \"ContextualVersionConflict ...\"というエラーが出たら、上部メニューの「ランタイム」から「ランタイムを再起動」をクリックして、ランタイムを再起動してみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5jhMBOX72rt"
      },
      "source": [
        "## 1. 日本語NLPのためのspaCyの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTdXc5Z_7UZ_"
      },
      "source": [
        "### spaCyを最新版にアップデート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDyU7FXpgf6L"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm9xRGBQ82x9"
      },
      "source": [
        "* バージョンを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQzgkN1B8yzP"
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOdfbC087sA0"
      },
      "source": [
        "### spaCyの日本語の訓練済み統計モデルをダウンロードする\n",
        "* 「sm」で終わるものは、小規模なモデル。\n",
        "* 「md」で終わるものが、中規模のモデル。\n",
        "* 「lg」で終わるものが、大規模なモデル。\n",
        "* 詳細は、以下を参照。\n",
        " * https://explosion.ai/blog/spacy-v2-3\n",
        "* ダウンロード時間短縮のため、ここでは小規模モデルを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_9hXxtgcfI"
      },
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62mbrOSY8QJ9"
      },
      "source": [
        "### Sudachiによる形態素解析\n",
        "* spaCyの日本語統計モデルをインストールすると、自動的にSudachiがインストールされる。\n",
        " * https://github.com/WorksApplications/Sudachi\n",
        "* そこで、sudachipyコマンドを使って、コマンドラインで形態素解析を試してみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_mUBCBAgz1o"
      },
      "source": [
        "!echo \"すもももももももものうち\" | sudachipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnUvkLz8i2y"
      },
      "source": [
        "## 2. spaCyによる日本語NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mREm_MVK8m5q"
      },
      "source": [
        "### 日本語統計モデルのロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXDTlY-s0Hfp"
      },
      "source": [
        "* ここでランタイムを再起動する。\n",
        "* その後、以下を実行。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzXLkIsEerem"
      },
      "source": [
        "from spacy.lang.ja import Japanese\n",
        "\n",
        "# Load SudachiPy with split mode A (default)\n",
        "nlp = Japanese()\n",
        "\n",
        "# Load SudachiPy with split mode B\n",
        "#cfg = {\"split_mode\": \"B\"}\n",
        "#nlp = Japanese(meta={\"tokenizer\": {\"config\": cfg}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7nRxPnE_HhI"
      },
      "source": [
        "* 文章を作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbVAM7qvgW0n"
      },
      "source": [
        "doc = nlp('これはやっぱり相当冗長な日本語の文じゃないかなと思っていたのですが。')\n",
        "print(doc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jW0p4D3_sk8"
      },
      "source": [
        "### spaCyからSudachiを利用\n",
        "* 分かち書きされた単語そのもの以外の、品詞などの情報も表示させている。\n",
        " * tagに「形容動詞」は無く、その語幹が「形状詞」とされている。\n",
        " * 「じゃない」の「ない」は、tagでは「形容詞」だが、posでは「AUX」である。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNu4wuDP_qU-"
      },
      "source": [
        "for token in doc:\n",
        "  print(f'text:{token.text}, pos:{token.pos_}, tag:{token.tag_}, lemma:{token.lemma_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tME6lbWjErVS"
      },
      "source": [
        "# 課題３\n",
        "\n",
        "* （この課題は、宿題ではなく、いまここで作業しつつ解いてしまいます。）\n",
        "* Wikipediaの複数の記事を、lemmaを半角スペースでつないだ、長い文字列へ変換する。\n",
        " * ここでは、コンピュータ科学の様々な分野の記事を題材として使う。\n",
        "* scikit-learnのCountVectorizerやTfidfVectorizerを使って、各記事における単語の出現頻度からなる文書ベクトルを得る。\n",
        "* 特徴ベクトルどうしの類似度を計算し、「人工知能」分野と最も似ている順に　３つの分野がどの分野かを求める。\n",
        " * 答えは自分の感覚でチェック。\n",
        " * 文書ベクトルを作る時に、単語の品詞を名詞に限定するなど、品詞の情報を使うことで結果を改善できるかどうかも、余裕があれば試行錯誤する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n4zl1W407u6"
      },
      "source": [
        "## 課題の手順(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhCnUa1syjI5"
      },
      "source": [
        "### Wikipediaのエントリをダウンロードして形態素解析を適用\n",
        "\n",
        "* Wikipediaの「人工知能」エントリをダウンロードする。\n",
        " * https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD\n",
        "* そして、そこに含まれる段落（__`<p>`__タグで囲まれた範囲）を列挙する。\n",
        "* 各段落のテキストに形態素解析を適用する。\n",
        "* 形態素解析で得られたlemmaを半角スペースでつないで、エントリ全体をひとつの長い文字列にする。\n",
        " * text、つまり、単語に分たれたそのままの文字列を半角スペースでつなぐのではない。\n",
        " * lemma（原型に戻したもの）を半角スペースでつなぐこと。\n",
        " * posやtagを見て、不要そうな単語を適当に削除してもよい。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ2gPVtnDZvk"
      },
      "source": [
        "# 「人工知能」エントリをダウンロードしてparserのインスタンスを作る。\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "url = 'https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD'\n",
        "html = urlopen(url) \n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y5MSWBXDuE-"
      },
      "source": [
        "# 段落のテキストを取得する。\n",
        "\n",
        "lines = list()\n",
        "for para in soup.find_all('p'):\n",
        "  lines.append(para.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CafzVR3NHG0c"
      },
      "source": [
        "lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXP8_h7v-Yb"
      },
      "source": [
        "# Sudachiで形態素解析し、分かち書き後のlemmaを取得する。\n",
        "\n",
        "x_pos = ['SPACE', 'PUNCT', 'AUX', 'ADP', 'SYM', 'DET', 'SCONJ', 'PART']\n",
        "tokens = list()\n",
        "for line in lines[:10]:\n",
        "  for token in nlp(line):\n",
        "    pos = token.pos_\n",
        "    if not pos in x_pos:\n",
        "      print(f'text:{token.text}, pos:{token.pos_}, tag:{token.tag_}, lemma:{token.lemma_}')\n",
        "      tokens.append(token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4dxDREcwZOi"
      },
      "source": [
        "# すべてのlemmaを半角スペースでつないで、長い文字列にする。\n",
        "\n",
        "doc_AI = ' '.join(tokens)\n",
        "print(doc_AI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcEH8FkCKUd4"
      },
      "source": [
        "* sudachiの設定を変えて使ってみる\n",
        " * https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/P8-5.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfv_gJ8RKUJx"
      },
      "source": [
        "# Load SudachiPy with split mode C\n",
        "cfg = {\"split_mode\": \"C\"}\n",
        "nlp = Japanese(meta={\"tokenizer\": {\"config\": cfg}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQJFIhsYKZUE"
      },
      "source": [
        "# Sudachiで形態素解析し、分かち書き後のlemmaを取得する。\n",
        "\n",
        "x_pos = ['SPACE', 'PUNCT', 'AUX', 'ADP', 'SYM', 'DET', 'SCONJ', 'PART']\n",
        "tokens = list()\n",
        "for line in lines[:10]:\n",
        "  for token in nlp(line):\n",
        "    pos = token.pos_\n",
        "    if pos not in x_pos:\n",
        "      print(f'text:{token.text}, pos:{token.pos_}, tag:{token.tag_}, lemma:{token.lemma_}')\n",
        "      tokens.append(token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnuFm2f7K2Rn"
      },
      "source": [
        "# すべてのlemmaを半角スペースでつないで、長い文字列にする。\n",
        "\n",
        "doc_AI = ' '.join(tokens)\n",
        "print(doc_AI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M94JaaA1zWRB"
      },
      "source": [
        "* 上記の操作をまとめておこなう関数を定義しておく。\n",
        " * 後で、各エントリについて、この関数を呼び出す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAJ5fUnTzV7w"
      },
      "source": [
        "def morph(soup, nlp):\n",
        "\n",
        "  lines = list()\n",
        "  for para in soup.find_all('p'):\n",
        "    lines.append(para.text)\n",
        "\n",
        "  x_pos = ['SPACE', 'PUNCT', 'AUX', 'ADP', 'SYM', 'DET', 'SCONJ', 'PART']\n",
        "  tokens = list()\n",
        "  for line in lines:\n",
        "    for token in nlp(line):\n",
        "      pos = token.pos_\n",
        "      if pos not in x_pos:\n",
        "        tokens.append(token.lemma_)\n",
        "\n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uha2LvYf1LZd"
      },
      "source": [
        "## 課題の手順(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAByKozFtYwF"
      },
      "source": [
        "### 「コンピュータ科学」の様々な分野に関するWikipediaエントリについて上記の作業を実行\n",
        "*  「人工知能」エントリの下部にある「コンピュータ科学」の分野一覧から、aタグのhref属性にあるURLを抜き出す。\n",
        "* ただし、__`/wiki/`__という文字列で始まっているURLであり、かつ、テンプレートの状態でないものだけを抜き出す。\n",
        " * Wikipediaのクローリングについてもっと詳しく知りたい場合は、下記のページ等を参照されたい。\n",
        "   * https://medium.com/@robinlphood/tutorial-a-simple-crawler-for-wikipedia-d7b6f6f55d5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qTCAg07qwcn"
      },
      "source": [
        "# 「人工知能」エントリの下部にある「コンピュータ科学」の分野一覧から、aタグのhref属性にあるURLを抜き出す。\n",
        "\n",
        "target_str = '表話編歴コンピュータ科学'\n",
        "prefix = '/wiki/'\n",
        "\n",
        "urls = dict()\n",
        "for table in soup.find_all('table'):\n",
        "  if table.text[:len(target_str)] != target_str: continue\n",
        "  for td in table.find_all('td'):\n",
        "    for a in td.find_all('a'):\n",
        "      if not a.text: continue\n",
        "      try:\n",
        "        if a.text.find('英語版') == -1:\n",
        "          href = a['href']\n",
        "          if href[:len(prefix)] == prefix and href.find('/Template:') == -1 and href.find('/Category:') == -1:\n",
        "            urls[a.text] = 'https://ja.wikipedia.org' + href\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "for k in urls:\n",
        "  print(k, urls[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luy0M3LjBd63"
      },
      "source": [
        "# 各エントリをダウンロードしてparserのインスタンスを作り、辞書として保存。\n",
        "\n",
        "soups = dict()\n",
        "for k in urls:\n",
        "  html = urlopen(urls[k]) \n",
        "  soups[k] = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_FMR37hvTHZ"
      },
      "source": [
        "# 先ほど定義した関数morph()を使って各エントリを形態素解析し、lemmaが半角スペースで区切られた文字列へ変換する。\n",
        "\n",
        "genre = list()\n",
        "corpus = list()\n",
        "for k in soups:\n",
        "  genre.append(k)\n",
        "  doc = morph(soups[k], nlp)\n",
        "  corpus.append(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZKoPJXNNgO"
      },
      "source": [
        "# 再利用するために、csvファイルとして保存しておく。\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "genre.append('人工知能')\n",
        "corpus.append(doc_AI)\n",
        "\n",
        "df = pd.DataFrame(list(zip(genre, corpus)), columns=['genre', 'text'])\n",
        "print(df.head())\n",
        "df.to_csv('cs_corpus.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIiKbJu1Ps9"
      },
      "source": [
        "## 課題の手順(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnSE8kLf1Re6"
      },
      "source": [
        "### scikit-learnのCountVectorizerで単語の出現頻度を要素とするベクトルを作成\n",
        "* これにより、各文書のベクトル表現が得られる。\n",
        "* 興味のある対象のベクトル表現を得ることは、その対象を機械学習アルゴリズムの入力データとして使うための第一歩。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmW1-x5zs99"
      },
      "source": [
        "# 先ほど作成したcsvファイルを読んで、textカラムをCountVectorizerのインスタンスでベクトル化する。\n",
        "# ベクトルの要素は、各Wikipediaエントリにおける、各単語の出現回数になる。\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df = pd.read_csv('cs_corpus.csv')\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTSpEuWM1g3d"
      },
      "source": [
        "# CountVectorizerで得られるのはsparse matrixである。\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9HpWKob5BSi"
      },
      "source": [
        "# sparse matrixから、通常のNumPyの配列へ変換する。\n",
        "X = X.toarray()\n",
        "print('文書数:{}; 語彙サイズ：{}'.format(*X.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLut43Mj1jDh"
      },
      "source": [
        "# CountVectorizerによって作られた語彙を取得する。\n",
        "vocab = vectorizer.get_feature_names()\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Mcat9s16Gk"
      },
      "source": [
        "# コーパス全体での出現頻度順で上位20単語を見てみる。\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "vocab = np.array(vocab)\n",
        "print(vocab[np.argsort(- X.sum(axis=0))][:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iVfMub3Aon"
      },
      "source": [
        "# 「人工知能」エントリと他のエントリとの距離を求める。\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "genre = df['genre'].values.tolist()\n",
        "index_AI = genre.index('人工知能')\n",
        "print(f'「{genre[index_AI]}」と「{genre[10]}」との間での・・・')\n",
        "print(f'ユークリッド距離: {np.linalg.norm(X[0] - X[index_AI])}')\n",
        "print(f'内積: {np.dot(X[0], X[index_AI])}')\n",
        "print(f'コサイン類似度: {distance.cosine(X[0], X[index_AI])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfb2TM94L-sC"
      },
      "source": [
        "### ここから各自作業。\n",
        "* 「人工知能」と、Wikipediaのエントリの類似度の上で、最も近いジャンルは、どれ？\n",
        "* ユークリッド距離とコサイン類似度のうち、どちらを類似度として使った場合の答えが、より納得できる答えになっているか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxRg04cV47EX"
      },
      "source": [
        "genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-da3ID_L0nK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}