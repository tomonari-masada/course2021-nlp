{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_PyTorch_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ju5Lc6g45t9qjI0R2cFUAA1wHBFm2IeB",
      "authorship_tag": "ABX9TyPtKAwsu9P8LKjM4peCyT7V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/07_PyTorch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hx2zN0IvyqF"
      },
      "source": [
        "# PyTorch入門 (3)\n",
        "* IMDbデータセットの感情分析をPyTorchを使っておこなう。\n",
        " * \b前にscikit-learnを使って同じ作業をおこなった。\n",
        "* 参考資料\n",
        " * PyTorch公式のチュートリアル https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* データは以前作ったIMDbの文書埋め込みを使う。\n",
        "* sentiment analysisのもっと高度な手法については、下記リンク先を参照。\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-hkFIH1xHUX"
      },
      "source": [
        "## 1. fastTextによる文書埋め込みをMLPの入力として使うための準備\n",
        "* MLP(多層パーセプトロン)の学習ぐらいは、空気を吸ったり吐いたりするぐらい自然にできるようにしておこう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0n2qW-6tsNC"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv8yYt8-Yn0"
      },
      "source": [
        "* （あらかじめランタイムのタイプをGPUに設定しておこう。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-jw4S8rhTB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyTNFT_uZBVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cca21a5-f070-4992-b687-e9925fbcb5d4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 26 14:45:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    32W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETpwV-Lmw-K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0541302-eff7-4823-94d5-ec23a2f4b5ba"
      },
      "source": [
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jB8UO3e9-_t"
      },
      "source": [
        "### 単語埋め込みデータファイルの読み込み\n",
        "* データファイルの準備の仕方\n",
        " * Blackboardで「自然言語処理特論」へ行く。\n",
        " * 「教材/課題/テスト」→「data」と順にクリックする。\n",
        " * 「IMDB dataset」のところに見えている4つの「.npy」ファイルをダウンロードする。\n",
        " * ダウンロードした4つのファイルを、自分のGoogle Driveの適当な場所にアップロードする。\n",
        " * 次のセルで、その置き場所を指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjF1vzSosL5l"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/2021Courses/NLP/'\n",
        "\n",
        "texts = dict()\n",
        "labels = dict()\n",
        "for tag in ['train', 'test']:\n",
        "  with open(f'{PATH}{tag}.npy', 'rb') as f:\n",
        "    texts[tag] = np.load(f)\n",
        "  with open(f'{PATH}{tag}_labels.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE7RXSjEsWPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e260e4f9-4ca2-49b6-f0c7-d4c39272eb81"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  print(texts[tag].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 300)\n",
            "(25000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuEe746MtFtr"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  texts[tag], labels[tag] = torch.tensor(texts[tag]), torch.tensor(labels[tag])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0QVlHKJ-vZn"
      },
      "source": [
        "## 2. 学習のための準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2OuR_1qeQi"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1osG09aHppg1"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "train_valid = MyDataset(texts['train'], labels['train'])\n",
        "test = MyDataset(texts['test'], labels['test'])\n",
        "\n",
        "valid_size = len(train_valid) // 5\n",
        "train_size = len(train_valid) - valid_size\n",
        "train, valid = random_split(train_valid,\n",
        "                            [train_size, valid_size],\n",
        "                            generator=torch.Generator().manual_seed(42)\n",
        "                            )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq2udez-0ez"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfloEv0Osj2n"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# 訓練データだけシャッフル\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-wx62k9d3J"
      },
      "source": [
        "## 3. モデルの定義と学習の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDOQzmVr0N6"
      },
      "source": [
        "### モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRnetIrs9DX"
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class):\n",
        "    super(TextSentiment, self).__init__()\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLkGnlqfvQpp"
      },
      "source": [
        "EMBED_DIM = texts['train'].size(1)\n",
        "NUM_CLASS = len(np.unique(labels['train']))\n",
        "model = TextSentiment(EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqF_wxjZkPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579b84a8-ab7d-4705-f47b-06abbb3c1703"
      },
      "source": [
        "print(EMBED_DIM, NUM_CLASS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWTIEYy_FKj"
      },
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWxfoJSzpKp"
      },
      "source": [
        "* 損失関数を除いて、以下の設定はいい加減なので、自分で調整してみよう。\n",
        "* schedulerの使い方は、調べてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB0NIxhlxka5"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,50], gamma=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W9fIHDa9j0P"
      },
      "source": [
        "## 4. 分類器の訓練と評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ARJYib_M84"
      },
      "source": [
        "### 評価を行なう関数\n",
        "* 正解率で評価する関数を定義しておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDFVMgRqyl4Q"
      },
      "source": [
        "def eval(model, criterion, loader):\n",
        "  model.eval()\n",
        "  \n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  total_size = 0\n",
        "  for input, target in loader:\n",
        "    with torch.no_grad():\n",
        "      input, target = input.to(device), target.to(device)\n",
        "      output = model(input)\n",
        "      loss = criterion(output, target)\n",
        "      total_loss += loss.item() * len(target)\n",
        "      total_acc += (output.argmax(1) == target).float().sum().item()\n",
        "      total_size += len(target)\n",
        "\n",
        "  return total_loss / total_size, total_acc / total_size"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ko1VZz_Kng"
      },
      "source": [
        "### 訓練を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvS00cVvaVU"
      },
      "source": [
        "def train(model, criterion, optimizer, train_loader, valid_loader, n_epochs=100):\n",
        "  model.train()\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for input, target in train_loader:\n",
        "      output = model(input.to(device))\n",
        "      loss = criterion(output, target.to(device))\n",
        "      train_loss += loss.item() * len(target) # 表示用の集計\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    valid_loss, valid_acc = eval(model, criterion, valid_loader)\n",
        "\n",
        "    # logging\n",
        "    print(f'epoch {epoch + 1:6d} |',\n",
        "          f'train loss {train_loss / train_size:8.4f} |',\n",
        "          f'valid loss {valid_loss:8.4f} | valid acc {valid_acc:8.3f}')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAiT90S4qae"
      },
      "source": [
        "### 訓練と評価の実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXpaWNwxqWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8fad0c-bcc6-40ed-880e-b8683ae44bab"
      },
      "source": [
        "train(model, criterion, optimizer, train_loader, valid_loader, 100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch      1 | train loss   0.4947 | valid loss   0.3888 | valid acc    0.834\n",
            "epoch      2 | train loss   0.3683 | valid loss   0.3711 | valid acc    0.845\n",
            "epoch      3 | train loss   0.3539 | valid loss   0.3770 | valid acc    0.840\n",
            "epoch      4 | train loss   0.3495 | valid loss   0.3607 | valid acc    0.847\n",
            "epoch      5 | train loss   0.3452 | valid loss   0.3557 | valid acc    0.850\n",
            "epoch      6 | train loss   0.3391 | valid loss   0.3552 | valid acc    0.853\n",
            "epoch      7 | train loss   0.3374 | valid loss   0.3764 | valid acc    0.829\n",
            "epoch      8 | train loss   0.3378 | valid loss   0.3641 | valid acc    0.839\n",
            "epoch      9 | train loss   0.3330 | valid loss   0.3588 | valid acc    0.845\n",
            "epoch     10 | train loss   0.3315 | valid loss   0.3484 | valid acc    0.854\n",
            "epoch     11 | train loss   0.3328 | valid loss   0.3469 | valid acc    0.855\n",
            "epoch     12 | train loss   0.3288 | valid loss   0.3457 | valid acc    0.854\n",
            "epoch     13 | train loss   0.3280 | valid loss   0.3469 | valid acc    0.854\n",
            "epoch     14 | train loss   0.3254 | valid loss   0.3495 | valid acc    0.853\n",
            "epoch     15 | train loss   0.3221 | valid loss   0.3440 | valid acc    0.855\n",
            "epoch     16 | train loss   0.3236 | valid loss   0.3456 | valid acc    0.854\n",
            "epoch     17 | train loss   0.3199 | valid loss   0.3489 | valid acc    0.849\n",
            "epoch     18 | train loss   0.3153 | valid loss   0.3489 | valid acc    0.849\n",
            "epoch     19 | train loss   0.3165 | valid loss   0.3412 | valid acc    0.856\n",
            "epoch     20 | train loss   0.3131 | valid loss   0.3443 | valid acc    0.855\n",
            "epoch     21 | train loss   0.3093 | valid loss   0.3440 | valid acc    0.856\n",
            "epoch     22 | train loss   0.3093 | valid loss   0.3435 | valid acc    0.852\n",
            "epoch     23 | train loss   0.3087 | valid loss   0.3415 | valid acc    0.858\n",
            "epoch     24 | train loss   0.3070 | valid loss   0.3440 | valid acc    0.855\n",
            "epoch     25 | train loss   0.3065 | valid loss   0.3409 | valid acc    0.859\n",
            "epoch     26 | train loss   0.3047 | valid loss   0.3398 | valid acc    0.857\n",
            "epoch     27 | train loss   0.2996 | valid loss   0.3476 | valid acc    0.855\n",
            "epoch     28 | train loss   0.2958 | valid loss   0.3449 | valid acc    0.852\n",
            "epoch     29 | train loss   0.2954 | valid loss   0.3421 | valid acc    0.854\n",
            "epoch     30 | train loss   0.2942 | valid loss   0.3409 | valid acc    0.857\n",
            "epoch     31 | train loss   0.2917 | valid loss   0.3420 | valid acc    0.857\n",
            "epoch     32 | train loss   0.2883 | valid loss   0.3433 | valid acc    0.858\n",
            "epoch     33 | train loss   0.2851 | valid loss   0.3433 | valid acc    0.854\n",
            "epoch     34 | train loss   0.2836 | valid loss   0.3455 | valid acc    0.854\n",
            "epoch     35 | train loss   0.2792 | valid loss   0.3486 | valid acc    0.856\n",
            "epoch     36 | train loss   0.2797 | valid loss   0.3476 | valid acc    0.858\n",
            "epoch     37 | train loss   0.2752 | valid loss   0.3523 | valid acc    0.848\n",
            "epoch     38 | train loss   0.2739 | valid loss   0.3625 | valid acc    0.847\n",
            "epoch     39 | train loss   0.2720 | valid loss   0.3527 | valid acc    0.858\n",
            "epoch     40 | train loss   0.2678 | valid loss   0.3559 | valid acc    0.853\n",
            "epoch     41 | train loss   0.2608 | valid loss   0.3659 | valid acc    0.850\n",
            "epoch     42 | train loss   0.2637 | valid loss   0.3615 | valid acc    0.849\n",
            "epoch     43 | train loss   0.2619 | valid loss   0.3497 | valid acc    0.859\n",
            "epoch     44 | train loss   0.2568 | valid loss   0.3497 | valid acc    0.859\n",
            "epoch     45 | train loss   0.2542 | valid loss   0.3641 | valid acc    0.848\n",
            "epoch     46 | train loss   0.2516 | valid loss   0.3889 | valid acc    0.844\n",
            "epoch     47 | train loss   0.2499 | valid loss   0.3579 | valid acc    0.854\n",
            "epoch     48 | train loss   0.2436 | valid loss   0.3584 | valid acc    0.858\n",
            "epoch     49 | train loss   0.2430 | valid loss   0.3515 | valid acc    0.860\n",
            "epoch     50 | train loss   0.2360 | valid loss   0.3650 | valid acc    0.851\n",
            "epoch     51 | train loss   0.2350 | valid loss   0.3743 | valid acc    0.854\n",
            "epoch     52 | train loss   0.2315 | valid loss   0.3797 | valid acc    0.842\n",
            "epoch     53 | train loss   0.2314 | valid loss   0.3726 | valid acc    0.853\n",
            "epoch     54 | train loss   0.2224 | valid loss   0.3717 | valid acc    0.852\n",
            "epoch     55 | train loss   0.2208 | valid loss   0.4137 | valid acc    0.828\n",
            "epoch     56 | train loss   0.2189 | valid loss   0.3813 | valid acc    0.853\n",
            "epoch     57 | train loss   0.2145 | valid loss   0.3842 | valid acc    0.852\n",
            "epoch     58 | train loss   0.2165 | valid loss   0.3775 | valid acc    0.857\n",
            "epoch     59 | train loss   0.2099 | valid loss   0.4034 | valid acc    0.848\n",
            "epoch     60 | train loss   0.2055 | valid loss   0.3764 | valid acc    0.850\n",
            "epoch     61 | train loss   0.2012 | valid loss   0.4018 | valid acc    0.847\n",
            "epoch     62 | train loss   0.1934 | valid loss   0.4107 | valid acc    0.853\n",
            "epoch     63 | train loss   0.1931 | valid loss   0.4066 | valid acc    0.845\n",
            "epoch     64 | train loss   0.2007 | valid loss   0.4267 | valid acc    0.839\n",
            "epoch     65 | train loss   0.1868 | valid loss   0.4144 | valid acc    0.854\n",
            "epoch     66 | train loss   0.1808 | valid loss   0.4236 | valid acc    0.847\n",
            "epoch     67 | train loss   0.1741 | valid loss   0.4387 | valid acc    0.849\n",
            "epoch     68 | train loss   0.1737 | valid loss   0.4615 | valid acc    0.854\n",
            "epoch     69 | train loss   0.1679 | valid loss   0.4659 | valid acc    0.849\n",
            "epoch     70 | train loss   0.1625 | valid loss   0.4408 | valid acc    0.847\n",
            "epoch     71 | train loss   0.1596 | valid loss   0.4526 | valid acc    0.849\n",
            "epoch     72 | train loss   0.1542 | valid loss   0.4905 | valid acc    0.845\n",
            "epoch     73 | train loss   0.1502 | valid loss   0.4696 | valid acc    0.844\n",
            "epoch     74 | train loss   0.1489 | valid loss   0.5016 | valid acc    0.843\n",
            "epoch     75 | train loss   0.1468 | valid loss   0.5041 | valid acc    0.837\n",
            "epoch     76 | train loss   0.1399 | valid loss   0.5186 | valid acc    0.842\n",
            "epoch     77 | train loss   0.1377 | valid loss   0.5022 | valid acc    0.841\n",
            "epoch     78 | train loss   0.1284 | valid loss   0.5044 | valid acc    0.835\n",
            "epoch     79 | train loss   0.1333 | valid loss   0.5327 | valid acc    0.843\n",
            "epoch     80 | train loss   0.1278 | valid loss   0.5551 | valid acc    0.841\n",
            "epoch     81 | train loss   0.1231 | valid loss   0.5500 | valid acc    0.838\n",
            "epoch     82 | train loss   0.1152 | valid loss   0.5433 | valid acc    0.838\n",
            "epoch     83 | train loss   0.1178 | valid loss   0.6038 | valid acc    0.840\n",
            "epoch     84 | train loss   0.1066 | valid loss   0.6071 | valid acc    0.844\n",
            "epoch     85 | train loss   0.1077 | valid loss   0.5712 | valid acc    0.842\n",
            "epoch     86 | train loss   0.1020 | valid loss   0.6070 | valid acc    0.837\n",
            "epoch     87 | train loss   0.1005 | valid loss   0.6064 | valid acc    0.842\n",
            "epoch     88 | train loss   0.1050 | valid loss   0.6419 | valid acc    0.840\n",
            "epoch     89 | train loss   0.0923 | valid loss   0.6360 | valid acc    0.835\n",
            "epoch     90 | train loss   0.0985 | valid loss   0.6401 | valid acc    0.825\n",
            "epoch     91 | train loss   0.0914 | valid loss   0.6584 | valid acc    0.842\n",
            "epoch     92 | train loss   0.0903 | valid loss   0.7069 | valid acc    0.842\n",
            "epoch     93 | train loss   0.0819 | valid loss   0.7219 | valid acc    0.836\n",
            "epoch     94 | train loss   0.0789 | valid loss   0.6585 | valid acc    0.835\n",
            "epoch     95 | train loss   0.0723 | valid loss   0.6943 | valid acc    0.835\n",
            "epoch     96 | train loss   0.0678 | valid loss   0.7655 | valid acc    0.841\n",
            "epoch     97 | train loss   0.0750 | valid loss   0.7198 | valid acc    0.833\n",
            "epoch     98 | train loss   0.0707 | valid loss   0.7123 | valid acc    0.832\n",
            "epoch     99 | train loss   0.0709 | valid loss   0.8207 | valid acc    0.826\n",
            "epoch    100 | train loss   0.0630 | valid loss   0.7492 | valid acc    0.831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gLwC3XDdK3-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbdnML0AuQv1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGDP7J3srA-d"
      },
      "source": [
        "## 5. 単語埋め込みもパラメータになっているモデル\n",
        "* fasttextの単語埋め込みを使うのをやめる。\n",
        "* 単語埋め込みも同時に学習することにする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM4Ku5bQrJlr"
      },
      "source": [
        "### IMDbデータセットをテキストデータとして読み直す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkj_D4IIeDvd",
        "outputId": "54fc769a-ce8a-4397-db77-36e95111a602"
      },
      "source": [
        "!pip install ml_datasets"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_datasets\n",
            "  Downloading ml_datasets-0.2.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ml_datasets) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ml_datasets) (4.62.3)\n",
            "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ml_datasets) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from ml_datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<3.0.0,>=0.2.0->ml_datasets) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<3.0.0,>=0.2.0->ml_datasets) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<3.0.0,>=0.2.0->ml_datasets) (3.6.0)\n",
            "Installing collected packages: ml-datasets\n",
            "Successfully installed ml-datasets-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoXbgo88eFsb",
        "outputId": "e77a8880-5953-46aa-a132-c57fa9fd48a0"
      },
      "source": [
        "from ml_datasets import imdb\n",
        "train_data, test_data = imdb()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "84131840it [00:49, 1716148.48it/s]                             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Untaring file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saAM3Wp3eHg9"
      },
      "source": [
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "dgmne7DYeKTv",
        "outputId": "1807ea47-182e-4193-de9e-aebdedf6a150"
      },
      "source": [
        "train_texts[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Another turgid action/adventure flick from the Quinn Martin Productions factory. Roy Thinnes plays undercover agent Diamond Head (Mr. Head, to you), working for his G-Man handler \"Aunt Mary\", looking for \"Tree\", who\\'s on a mission to...well, just watch the movie. \\n\\n\\n\\nThis one deserved and got the full MST3K sendup. As the boys and various reviewers have pointed out, the movie \"Fargo\" had more Hawaiian locations than this film. Apparently shot on a puny budget, this movie highlights Hawaii\\'s broken-down dive shops, gas stations, and cheapo hotels. Zulu -- later to star as Kono in Hawaii-Five-O -- appears as Thinnes\\' lumpy, inept sidekick, while France Nguyen models the Jenny Craig diet gone horribly wrong. Others sharing the flickering screen include a drunken Richard Harris knockoff, a George Takai imitator, a not-so-smart hit-man with sprayed-on Sansabelt slacks, and the villain \"Tree\", sporting a veddy British accent. You can pretty much figure out the plot halfway through the opening credits, but relax--just enjoy the giddy mediocrity of this 70\\'s movie-of-the-week.\\n\\n\\n\\nWhenever I think of this movie (and I think of this movie often), I catch myself humming the theme, written for flute and tuba...no one knows why. \\n\\n\\n\\nTrivia note--Diamond Head was directed by Jeannot Szwarc, one of three contract directors at Universal who would go on to make much bigger films, in his case Jaws 2. The others were John Badham (War Games), and a young fellow named Steven Spielberg...'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pcL7GnJ_kLSA",
        "outputId": "8cb843e5-949d-4598-b235-5b04e8d0ad40"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neg'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao2-FVU3rT04"
      },
      "source": [
        "### ラベルを0/1の整数に変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6WVH_sKkRfF"
      },
      "source": [
        "unique_labels = np.unique(train_labels)\n",
        "label_id = {}\n",
        "for i, label in enumerate(unique_labels):\n",
        "  label_id[label] = i"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTfw1B-pkriP"
      },
      "source": [
        "train_labels = [label_id[label] for label in train_labels]\n",
        "test_labels = [label_id[label] for label in test_labels]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU9OGXwEk4SX",
        "outputId": "210bec0d-4105-4764-8072-e425e6834ef7"
      },
      "source": [
        "print(train_labels[:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bjc7f1mrYT5"
      },
      "source": [
        "### sklearnのCountVectorizerを使ってトークン化\n",
        "* `torchtext`を使う方法は後日説明。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbx6PzxvriMt"
      },
      "source": [
        "* 語彙集合の構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QywYL6Ise-5h",
        "outputId": "32712b70-d4b1-49d4-b92c-49b4a1771f5e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=10, max_df=0.2)\n",
        "vectorizer.fit(train_texts)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.2, min_df=10)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfDGUuTBf11l",
        "outputId": "ac73dc85-13e4-4038-ac89-a8856c8b615b"
      },
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "print([vocab[i] for i in range(10)])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00', '000', '007', '01', '02', '05', '06', '07', '08', '10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVZZrqEvQfx"
      },
      "source": [
        "* ある単語が語彙集合に入っているかどうかは、下のようにしてチェックできる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGN_JN9KgSWt",
        "outputId": "15fcbb01-abdd-4b0e-dba2-ca2465b08616"
      },
      "source": [
        "'to' in vectorizer.vocabulary_"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdzpT5R9rq5X"
      },
      "source": [
        "* preprocessorとtokenizerの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PETeskPfQXS"
      },
      "source": [
        "preprocessor = vectorizer.build_preprocessor()\n",
        "tokenizer = vectorizer.build_tokenizer()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rGTz2nTrthv"
      },
      "source": [
        "* トークン列をインデックス列に変換する関数\n",
        " * 単語のインデックスを、パディング用の単語と、未知語との２つ分、後ろにずらす。\n",
        " * テキストの長さを`max_len`に揃えるという作業も同時に行なう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaIJVmCffXEW"
      },
      "source": [
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "VOCAB_SIZE = len(vocab) + 2\n",
        "\n",
        "def encode(text, max_len=1000, padding_idx=PAD_IDX, unknown_idx=UNK_IDX):\n",
        "  idx_seq = []\n",
        "  for token in tokenizer(preprocessor(text)):\n",
        "    if token in vectorizer.vocabulary_:\n",
        "      idx_seq.append(vectorizer.vocabulary_[token])\n",
        "    else:\n",
        "      idx_seq.append(unknown_idx)\n",
        "  if len(idx_seq) < max_len:\n",
        "    idx_seq += [padding_idx] * (max_len - len(idx_seq))\n",
        "  else:\n",
        "    idx_seq = idx_seq[:max_len]\n",
        "  return idx_seq"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fYL82BDqNYm",
        "outputId": "e45a063f-6238-4c96-d18c-91039fe9bd21"
      },
      "source": [
        "print(VOCAB_SIZE)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSGwirhyhFfe",
        "outputId": "450a1352-363c-4392-e1e1-73a02ae3e71e"
      },
      "source": [
        "print(encode(train_texts[0]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[892, 17055, 400, 506, 6487, 1, 1, 13015, 10169, 12714, 6093, 14025, 1, 12270, 17181, 581, 4682, 7682, 10832, 7682, 1, 1, 18217, 1, 1, 10051, 1, 1285, 10180, 9795, 1, 16930, 1, 1, 10604, 1, 1, 1, 1, 1, 1, 1, 1, 4571, 1, 7212, 1, 6815, 10836, 1, 1, 1, 2114, 1, 17535, 13739, 1, 12328, 1, 1, 1, 6168, 1, 1, 7661, 9753, 1, 1, 1, 963, 14754, 1, 1, 2304, 1, 1, 7857, 7660, 2245, 5073, 4947, 14745, 6934, 15617, 1, 2866, 8061, 18415, 9420, 1, 15578, 1, 1, 1, 7660, 6435, 973, 1, 1, 1, 8476, 14824, 18016, 6698, 1, 10655, 1, 8955, 3890, 4711, 7177, 8035, 18283, 11569, 14642, 1, 6489, 14351, 8394, 5200, 13790, 7614, 9270, 7013, 1, 1, 1, 1, 15094, 7910, 10051, 1, 15481, 1, 1, 1, 1, 1, 17661, 16930, 15466, 1, 2226, 327, 1, 1, 12626, 1, 6348, 1, 1, 1, 7505, 16591, 1, 11473, 3949, 1, 13433, 1, 5633, 1, 7049, 10329, 1, 1, 217, 1, 1, 1, 17944, 18005, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11415, 2666, 10918, 8119, 1, 16500, 18282, 1, 6542, 1, 1, 1, 1, 9281, 18059, 16987, 11240, 4682, 7682, 1, 4769, 1, 1, 1, 1, 1, 16576, 3679, 4777, 1, 17315, 1, 1, 7145, 1, 1, 1, 1, 1785, 6368, 1, 1, 2633, 8930, 1, 11569, 1, 9003, 1, 17833, 6895, 1, 18350, 6272, 10952, 15673, 15399, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPq9QZynr8ft"
      },
      "source": [
        "* バッチ単位でトークン列をインデックス列に変換する関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ZjuJcOl6pr"
      },
      "source": [
        "def batch_encode(texts):\n",
        "  sequences = []\n",
        "  for text in texts:\n",
        "    sequences.append(encode(text))\n",
        "  return torch.Tensor(sequences)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF5Vv9fCsEB3"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmAulBKiwpl"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class MyTextDataset(Dataset):\n",
        "  def __init__(self, texts, labels):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.texts[index], self.labels[index]\n",
        "\n",
        "train_valid_set = MyTextDataset(train_texts, train_labels)\n",
        "test_set = MyTextDataset(test_texts, test_labels)\n",
        "\n",
        "valid_size = len(train_valid) // 5\n",
        "train_size = len(train_valid) - valid_size\n",
        "train_set, valid_set = random_split(train_valid_set,\n",
        "                                    [train_size, valid_size],\n",
        "                                    generator=torch.Generator().manual_seed(42)\n",
        "                                    )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaWQOTx_sGIe"
      },
      "source": [
        "### DataLoader\n",
        "* ミニバッチのテキストをインデックス列へ変換するcollation用の関数も定義する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-rqRRyjmcmG"
      },
      "source": [
        "def collate_fn(batch):\n",
        "  batch_texts, batch_labels = zip(*batch)\n",
        "  return batch_encode(batch_texts).type(torch.LongTensor), torch.LongTensor(batch_labels)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAYc_ysrjbfP"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# 訓練データだけシャッフル\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0x3kqRcsTb7"
      },
      "source": [
        "### モデルの定義\n",
        "* `nn.Embedding`を使うところがポイント。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbiMoa3adP7E"
      },
      "source": [
        "class EmbeddedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx=PAD_IDX):\n",
        "    super(EmbeddedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.dropout(self.embed(text))\n",
        "    x = embedded.mean(1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljnfpme3v8TF"
      },
      "source": [
        "* モデルのインスタンスを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE_sBuU2iSMC"
      },
      "source": [
        "EMBED_DIM = 300\n",
        "NUM_CLASS = len(np.unique(train_labels))\n",
        "model = EmbeddedTextSentiment(EMBED_DIM, NUM_CLASS, VOCAB_SIZE, PAD_IDX).to(device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKWx0ApasYxc"
      },
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD61kYGMqa7I"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,50], gamma=0.1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPncv39nsb3s"
      },
      "source": [
        "### 学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "5BPcXFmejU3x",
        "outputId": "dff18ec3-3691-45d6-d373-e32c3f23ed95"
      },
      "source": [
        "train(model, criterion, optimizer, train_loader, valid_loader, 100)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch      1 | train loss   0.6892 | valid loss   0.6366 | valid acc    0.657\n",
            "epoch      2 | train loss   0.4393 | valid loss   0.3637 | valid acc    0.845\n",
            "epoch      3 | train loss   0.2783 | valid loss   0.3350 | valid acc    0.871\n",
            "epoch      4 | train loss   0.2252 | valid loss   0.3508 | valid acc    0.866\n",
            "epoch      5 | train loss   0.1800 | valid loss   0.3043 | valid acc    0.887\n",
            "epoch      6 | train loss   0.1506 | valid loss   0.3250 | valid acc    0.889\n",
            "epoch      7 | train loss   0.1158 | valid loss   0.3619 | valid acc    0.885\n",
            "epoch      8 | train loss   0.0920 | valid loss   0.3705 | valid acc    0.884\n",
            "epoch      9 | train loss   0.0797 | valid loss   0.3981 | valid acc    0.879\n",
            "epoch     10 | train loss   0.0624 | valid loss   0.4890 | valid acc    0.879\n",
            "epoch     11 | train loss   0.0468 | valid loss   0.5029 | valid acc    0.876\n",
            "epoch     12 | train loss   0.0350 | valid loss   0.5341 | valid acc    0.872\n",
            "epoch     13 | train loss   0.0265 | valid loss   0.5801 | valid acc    0.874\n",
            "epoch     14 | train loss   0.0213 | valid loss   0.6889 | valid acc    0.872\n",
            "epoch     15 | train loss   0.0173 | valid loss   0.7004 | valid acc    0.872\n",
            "epoch     16 | train loss   0.0143 | valid loss   0.6883 | valid acc    0.872\n",
            "epoch     17 | train loss   0.0308 | valid loss   0.6411 | valid acc    0.867\n",
            "epoch     18 | train loss   0.0215 | valid loss   0.8200 | valid acc    0.868\n",
            "epoch     19 | train loss   0.0119 | valid loss   0.9196 | valid acc    0.868\n",
            "epoch     20 | train loss   0.0085 | valid loss   0.8716 | valid acc    0.870\n",
            "epoch     21 | train loss   0.0073 | valid loss   0.8263 | valid acc    0.869\n",
            "epoch     22 | train loss   0.0064 | valid loss   0.9750 | valid acc    0.868\n",
            "epoch     23 | train loss   0.0064 | valid loss   0.9107 | valid acc    0.868\n",
            "epoch     24 | train loss   0.0050 | valid loss   1.0046 | valid acc    0.868\n",
            "epoch     25 | train loss   0.0321 | valid loss   0.6668 | valid acc    0.840\n",
            "epoch     26 | train loss   0.0157 | valid loss   0.8907 | valid acc    0.869\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-2ac4307df910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-cf5ae38018a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-a17fc29503a1>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mbatch_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-c100d9d277ff>\u001b[0m in \u001b[0;36mbatch_encode\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdegxekVwvCP",
        "outputId": "7989bae9-3c8b-45d6-9253-af364ace4425"
      },
      "source": [
        "loss, acc = eval(model, criterion, train_loader)\n",
        "print(f'train loss {loss:8.4f} | train acc {acc:8.3f}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss   0.0035 | train acc    1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzVkJ2v-tt_"
      },
      "source": [
        "# 課題\n",
        "* モデルやoptimizerやschedulerを変更して、validation setを使ってチューニングしよう。\n",
        "* 最後に、自分で選択した設定を使って、test set上で評価しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYPP13ByCid"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIFqTNLYba53"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26RYXnmnbbSc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnzANMG_vrT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6598acb-b8de-4823-d4e7-bd067808ebf3"
      },
      "source": [
        "loss, acc = eval(model, criterion, test_loader)\n",
        "print(f'test loss {loss:8.4f} | test acc {acc:8.3f}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss   0.7982 | test acc    0.834\n"
          ]
        }
      ]
    }
  ]
}