{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_PyTorch_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ju5Lc6g45t9qjI0R2cFUAA1wHBFm2IeB",
      "authorship_tag": "ABX9TyN3R+gYTuxBcm7UIUAKklrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/07_PyTorch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hx2zN0IvyqF"
      },
      "source": [
        "# 07 PyTorch入門 (3)\n",
        "* IMDbデータセットの感情分析をPyTorchを使っておこなう。\n",
        " * \b前にscikit-learnを使って同じ作業をおこなった。\n",
        "* 参考資料\n",
        " * PyTorch公式のチュートリアル https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* データは以前作ったIMDbの文書埋め込みを使う。\n",
        "* sentiment analysisのもっと高度な手法については、下記リンク先を参照。\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-hkFIH1xHUX"
      },
      "source": [
        "## 1. fastTextによる文書埋め込みをMLPの入力として使うための準備\n",
        "* MLP(多層パーセプトロン)の学習ぐらいは、空気を吸ったり吐いたりするぐらい自然にできるようにしておこう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jB8UO3e9-_t"
      },
      "source": [
        "### データファイルが置いてあるGoogle Driveのパスを変数PATHに設定\n",
        "* データファイルの扱い方\n",
        " * Blackboardで「自然言語処理特論」へ行く。\n",
        " * 「教材/課題/テスト」→「data」→「IMDb」と順にクリックする。\n",
        " * 見えている4つの「.npy」ファイルをダウンロードする。\n",
        " * ダウンロードした4つのファイルを、自分のGoogle Driveの適当な場所にアップロードする。\n",
        " * 次のセルで、その置き場所を指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P3MzvmdrobX"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/2021Courses/NLP/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv8yYt8-Yn0"
      },
      "source": [
        "あらかじめランタイムをGPUに設定しておこう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-jw4S8rhTB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETpwV-Lmw-K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadae2ef-12c9-41e4-8325-7e1c7c705224"
      },
      "source": [
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl9euZ-P-fiG"
      },
      "source": [
        "### 単語埋め込みデータファイルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjF1vzSosL5l"
      },
      "source": [
        "texts = dict()\n",
        "labels = dict()\n",
        "for tag in ['train', 'test']:\n",
        "  with open(f'{PATH}{tag}.npy', 'rb') as f:\n",
        "    texts[tag] = np.load(f)\n",
        "  with open(f'{PATH}{tag}_labels.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE7RXSjEsWPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fe7722-72ec-4637-f0f2-bff33686ab85"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  print(texts[tag].shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 300)\n",
            "(25000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7AybiDZ-jvr"
      },
      "source": [
        "### ndarrayをPyTorchのテンソルに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuEe746MtFtr"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  texts[tag], labels[tag] = torch.tensor(texts[tag]), torch.tensor(labels[tag])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0QVlHKJ-vZn"
      },
      "source": [
        "## 2. 学習のための準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2OuR_1qeQi"
      },
      "source": [
        "### 訓練データの分割\n",
        "* validation setを切り出すため。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1osG09aHppg1"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "train_valid = MyDataset(texts['train'], labels['train'])\n",
        "test = MyDataset(texts['test'], labels['test'])\n",
        "\n",
        "valid_size = len(train_valid) // 5\n",
        "train_size = len(train_valid) - valid_size\n",
        "train, valid = random_split(train_valid,\n",
        "                            [train_size, valid_size],\n",
        "                            generator=torch.Generator().manual_seed(42)\n",
        "                            )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq2udez-0ez"
      },
      "source": [
        "### データローダの作成\n",
        "* shuffleをTrueにして、毎エポック異なる順で訓練データを見ていくようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfloEv0Osj2n"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# 訓練データだけシャッフル\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-wx62k9d3J"
      },
      "source": [
        "## 3. モデルの定義と学習の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDOQzmVr0N6"
      },
      "source": [
        "### モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRnetIrs9DX"
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class):\n",
        "    super(TextSentiment, self).__init__()\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLkGnlqfvQpp"
      },
      "source": [
        "EMBED_DIM = texts['train'].shape[1]\n",
        "NUM_CLASS = len(np.unique(labels['train']))\n",
        "model = TextSentiment(EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWTIEYy_FKj"
      },
      "source": [
        "### 損失関数とoptimizerとschedulerを作る\n",
        "* https://pytorch.org/docs/stable/optim.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWxfoJSzpKp"
      },
      "source": [
        "* 損失関数を除いて、以下の設定は適当なので、各自、調整すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB0NIxhlxka5"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,50], gamma=0.1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W9fIHDa9j0P"
      },
      "source": [
        "## 4. 分類器の訓練と評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ARJYib_M84"
      },
      "source": [
        "### 評価用の関数\n",
        "* 正解率で評価する関数を定義しておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDFVMgRqyl4Q"
      },
      "source": [
        "def eval(loader):\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  total_size = 0\n",
        "  for input, target in loader:\n",
        "    with torch.no_grad():\n",
        "      input, target = input.to(device), target.to(device)\n",
        "      output = model(input)\n",
        "      loss = criterion(output, target)\n",
        "      total_loss += loss.item() * len(target)\n",
        "      total_acc += (output.argmax(1) == target).float().sum().item()\n",
        "      total_size += len(target)\n",
        "\n",
        "  return total_loss / total_size, total_acc / total_size"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ko1VZz_Kng"
      },
      "source": [
        "### 訓練のループ\n",
        "* 前回とほぼ同じ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvS00cVvaVU"
      },
      "source": [
        "def train(train_loader, valid_loader, n_epochs=100):\n",
        "  # training loop\n",
        "  for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    for input, target in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      # forward pass\n",
        "      output = model(input.to(device))\n",
        "      loss = criterion(output, target.to(device))\n",
        "      train_loss += loss.item() * len(target) # 表示用の集計\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "    # valid loss\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for input, target in valid_loader:\n",
        "        output = model(input.to(device))\n",
        "        loss = criterion(output, target.to(device))\n",
        "        valid_loss += loss.item() * len(target)\n",
        "\n",
        "    # logging\n",
        "    print(f'epoch {epoch + 1:4d} ;',\n",
        "          f'train loss {train_loss / train_size:8.4f} ;',\n",
        "          f'valid loss {valid_loss / valid_size:8.4f} ;')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAXpaWNwxqWN",
        "outputId": "33238559-7b3a-4f8a-d39f-6ab446c099c9"
      },
      "source": [
        "train(train_loader, valid_loader, 100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    1 ; train loss   0.6448 ; valid loss   0.6136 ;\n",
            "epoch    2 ; train loss   0.5947 ; valid loss   0.5793 ;\n",
            "epoch    3 ; train loss   0.5659 ; valid loss   0.5566 ;\n",
            "epoch    4 ; train loss   0.5450 ; valid loss   0.5400 ;\n",
            "epoch    5 ; train loss   0.5305 ; valid loss   0.5303 ;\n",
            "epoch    6 ; train loss   0.5174 ; valid loss   0.5156 ;\n",
            "epoch    7 ; train loss   0.5052 ; valid loss   0.5095 ;\n",
            "epoch    8 ; train loss   0.4946 ; valid loss   0.5028 ;\n",
            "epoch    9 ; train loss   0.4843 ; valid loss   0.4835 ;\n",
            "epoch   10 ; train loss   0.4755 ; valid loss   0.4769 ;\n",
            "epoch   11 ; train loss   0.4691 ; valid loss   0.4670 ;\n",
            "epoch   12 ; train loss   0.4614 ; valid loss   0.4604 ;\n",
            "epoch   13 ; train loss   0.4568 ; valid loss   0.4933 ;\n",
            "epoch   14 ; train loss   0.4491 ; valid loss   0.4811 ;\n",
            "epoch   15 ; train loss   0.4462 ; valid loss   0.4471 ;\n",
            "epoch   16 ; train loss   0.4404 ; valid loss   0.4427 ;\n",
            "epoch   17 ; train loss   0.4381 ; valid loss   0.4366 ;\n",
            "epoch   18 ; train loss   0.4312 ; valid loss   0.4323 ;\n",
            "epoch   19 ; train loss   0.4279 ; valid loss   0.4298 ;\n",
            "epoch   20 ; train loss   0.4301 ; valid loss   0.4285 ;\n",
            "epoch   21 ; train loss   0.4220 ; valid loss   0.4320 ;\n",
            "epoch   22 ; train loss   0.4224 ; valid loss   0.4470 ;\n",
            "epoch   23 ; train loss   0.4167 ; valid loss   0.4385 ;\n",
            "epoch   24 ; train loss   0.4145 ; valid loss   0.4257 ;\n",
            "epoch   25 ; train loss   0.4159 ; valid loss   0.4370 ;\n",
            "epoch   26 ; train loss   0.4136 ; valid loss   0.4257 ;\n",
            "epoch   27 ; train loss   0.4110 ; valid loss   0.4089 ;\n",
            "epoch   28 ; train loss   0.4065 ; valid loss   0.4070 ;\n",
            "epoch   29 ; train loss   0.4079 ; valid loss   0.4742 ;\n",
            "epoch   30 ; train loss   0.4114 ; valid loss   0.4033 ;\n",
            "epoch   31 ; train loss   0.4038 ; valid loss   0.4105 ;\n",
            "epoch   32 ; train loss   0.4011 ; valid loss   0.4009 ;\n",
            "epoch   33 ; train loss   0.3969 ; valid loss   0.4010 ;\n",
            "epoch   34 ; train loss   0.4016 ; valid loss   0.4529 ;\n",
            "epoch   35 ; train loss   0.3995 ; valid loss   0.4159 ;\n",
            "epoch   36 ; train loss   0.3961 ; valid loss   0.3966 ;\n",
            "epoch   37 ; train loss   0.3938 ; valid loss   0.3999 ;\n",
            "epoch   38 ; train loss   0.3941 ; valid loss   0.4176 ;\n",
            "epoch   39 ; train loss   0.3950 ; valid loss   0.3915 ;\n",
            "epoch   40 ; train loss   0.3926 ; valid loss   0.4589 ;\n",
            "epoch   41 ; train loss   0.3987 ; valid loss   0.4124 ;\n",
            "epoch   42 ; train loss   0.3914 ; valid loss   0.4204 ;\n",
            "epoch   43 ; train loss   0.3925 ; valid loss   0.3919 ;\n",
            "epoch   44 ; train loss   0.3843 ; valid loss   0.4002 ;\n",
            "epoch   45 ; train loss   0.3880 ; valid loss   0.3993 ;\n",
            "epoch   46 ; train loss   0.3910 ; valid loss   0.4351 ;\n",
            "epoch   47 ; train loss   0.3883 ; valid loss   0.3911 ;\n",
            "epoch   48 ; train loss   0.3846 ; valid loss   0.3868 ;\n",
            "epoch   49 ; train loss   0.3938 ; valid loss   0.4077 ;\n",
            "epoch   50 ; train loss   0.3936 ; valid loss   0.4098 ;\n",
            "epoch   51 ; train loss   0.3831 ; valid loss   0.4130 ;\n",
            "epoch   52 ; train loss   0.3776 ; valid loss   0.3812 ;\n",
            "epoch   53 ; train loss   0.3843 ; valid loss   0.4099 ;\n",
            "epoch   54 ; train loss   0.3821 ; valid loss   0.4044 ;\n",
            "epoch   55 ; train loss   0.3800 ; valid loss   0.3806 ;\n",
            "epoch   56 ; train loss   0.3787 ; valid loss   0.3787 ;\n",
            "epoch   57 ; train loss   0.3853 ; valid loss   0.3986 ;\n",
            "epoch   58 ; train loss   0.3777 ; valid loss   0.3850 ;\n",
            "epoch   59 ; train loss   0.3823 ; valid loss   0.3775 ;\n",
            "epoch   60 ; train loss   0.3766 ; valid loss   0.4173 ;\n",
            "epoch   61 ; train loss   0.3795 ; valid loss   0.3971 ;\n",
            "epoch   62 ; train loss   0.3781 ; valid loss   0.4934 ;\n",
            "epoch   63 ; train loss   0.3761 ; valid loss   0.3836 ;\n",
            "epoch   64 ; train loss   0.3765 ; valid loss   0.4043 ;\n",
            "epoch   65 ; train loss   0.3721 ; valid loss   0.3749 ;\n",
            "epoch   66 ; train loss   0.3747 ; valid loss   0.3835 ;\n",
            "epoch   67 ; train loss   0.3740 ; valid loss   0.4516 ;\n",
            "epoch   68 ; train loss   0.3761 ; valid loss   0.3793 ;\n",
            "epoch   69 ; train loss   0.3748 ; valid loss   0.4013 ;\n",
            "epoch   70 ; train loss   0.3726 ; valid loss   0.3804 ;\n",
            "epoch   71 ; train loss   0.3882 ; valid loss   0.3762 ;\n",
            "epoch   72 ; train loss   0.3672 ; valid loss   0.4020 ;\n",
            "epoch   73 ; train loss   0.3757 ; valid loss   0.3756 ;\n",
            "epoch   74 ; train loss   0.3731 ; valid loss   0.3964 ;\n",
            "epoch   75 ; train loss   0.3662 ; valid loss   0.3790 ;\n",
            "epoch   76 ; train loss   0.3782 ; valid loss   0.4247 ;\n",
            "epoch   77 ; train loss   0.3670 ; valid loss   0.3789 ;\n",
            "epoch   78 ; train loss   0.3706 ; valid loss   0.3765 ;\n",
            "epoch   79 ; train loss   0.3712 ; valid loss   0.3691 ;\n",
            "epoch   80 ; train loss   0.3657 ; valid loss   0.3727 ;\n",
            "epoch   81 ; train loss   0.3729 ; valid loss   0.3691 ;\n",
            "epoch   82 ; train loss   0.3757 ; valid loss   0.3916 ;\n",
            "epoch   83 ; train loss   0.3659 ; valid loss   0.3893 ;\n",
            "epoch   84 ; train loss   0.3661 ; valid loss   0.4078 ;\n",
            "epoch   85 ; train loss   0.3655 ; valid loss   0.4129 ;\n",
            "epoch   86 ; train loss   0.3659 ; valid loss   0.3776 ;\n",
            "epoch   87 ; train loss   0.3620 ; valid loss   0.3756 ;\n",
            "epoch   88 ; train loss   0.3656 ; valid loss   0.3665 ;\n",
            "epoch   89 ; train loss   0.3776 ; valid loss   0.3665 ;\n",
            "epoch   90 ; train loss   0.3669 ; valid loss   0.3757 ;\n",
            "epoch   91 ; train loss   0.3620 ; valid loss   0.3688 ;\n",
            "epoch   92 ; train loss   0.3708 ; valid loss   0.3793 ;\n",
            "epoch   93 ; train loss   0.3606 ; valid loss   0.3770 ;\n",
            "epoch   94 ; train loss   0.3650 ; valid loss   0.3651 ;\n",
            "epoch   95 ; train loss   0.3656 ; valid loss   0.4096 ;\n",
            "epoch   96 ; train loss   0.3575 ; valid loss   0.3670 ;\n",
            "epoch   97 ; train loss   0.3613 ; valid loss   0.3680 ;\n",
            "epoch   98 ; train loss   0.3590 ; valid loss   0.3678 ;\n",
            "epoch   99 ; train loss   0.3683 ; valid loss   0.4013 ;\n",
            "epoch  100 ; train loss   0.3555 ; valid loss   0.3673 ;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ie1nhKGxOMX"
      },
      "source": [
        "* テストセット上での正解率を調べてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzANMG_vrT6",
        "outputId": "d17861e7-d529-4672-d983-78a2394f3a18"
      },
      "source": [
        "test_loss, test_acc = eval(test_loader)\n",
        "print(f'test loss : {test_loss:8.4f} | test accuracy : {test_acc:.3f}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss :   0.3626 | test accuracy : 0.844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzVkJ2v-tt_"
      },
      "source": [
        "# 課題7\n",
        "* モデルやoptimizerやschedulerを変更して、dev setの分類性能をできるだけ向上させてみよう。\n",
        "* その後、自分で選択した設定を使って、最終的にtest setで評価しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYPP13ByCid"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}