{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_topic_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12MxXsGfePft6pAHKkHrg23Yb7t_RSKiN",
      "authorship_tag": "ABX9TyOarZk13VwbAjC9z3TPOmc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/11_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPj0E52Uf-se"
      },
      "source": [
        "# トピックモデリングの実践\n",
        "\n",
        "* トピックモデリングを、NMF(nonnegative matrix factorization)とLDA(latent Dirichlet allocation)とで実践\n",
        " * LDAの理屈については「統計モデリング２」で。\n",
        " * いずれもsklearnの実装を使う。\n",
        " * 各トピックの上位単語はワードクラウドで可視化する。\n",
        "\n",
        "* 入力データは、各文書における各単語の出現回数、またはTF-IDF\n",
        " * NMFやLDAはbag-of-wordsモデルなので、語順は考慮されない。\n",
        " * LDAの入力データとしては、出現回数を使う。\n",
        " * NMFの入力データは、出現回数でも、TF-IDFでも、どちらでも良い。\n",
        "\n",
        "* 参考資料\n",
        " * https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トピックモデルの使い方\n",
        " * 文書ベクトルの次元圧縮のための手法としては、いまひとつ性能が良くないかも。\n",
        " * EDA (exploratory data analysis) の手法として使うのが良いかも。"
      ],
      "metadata": {
        "id": "pESqkOlwZI_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDAの可視化ツールを先にインストール\n",
        "* pyLDAvisというツールをインストールすると、ランタイムの再起動が必要になるため"
      ],
      "metadata": {
        "id": "ecYHZBNFdEm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "0MQgRVZZdBdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGYHQvxffZW"
      },
      "source": [
        "## データセットの準備\n",
        "* NeurIPSで発表された1,740本の論文の本文を使う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96F3VXGWfypB"
      },
      "source": [
        "### データをダウンロードしリスト化する関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVxhV6BRfalH"
      },
      "source": [
        "import io\n",
        "import os.path\n",
        "import re\n",
        "import tarfile\n",
        "import smart_open\n",
        "\n",
        "\n",
        "PATH = '/content/drive/MyDrive/data'\n",
        "\n",
        "\n",
        "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
        "\n",
        "  fname = os.path.join(PATH, url.split('/')[-1])\n",
        "\n",
        "  if not os.path.isfile(fname):\n",
        "    with smart_open.open(url, \"rb\") as fin:\n",
        "      with smart_open.open(fname, 'wb') as fout:\n",
        "        while True:\n",
        "          buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
        "          if not buf:\n",
        "            break\n",
        "          fout.write(buf)\n",
        "\n",
        "  with tarfile.open(fname, mode='r:gz') as tar:\n",
        "  # Ignore directory entries, as well as files like README, etc.\n",
        "    files = [\n",
        "             m for m in tar.getmembers()\n",
        "             if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
        "             ]\n",
        "    for member in sorted(files, key=lambda x: x.name):\n",
        "      member_bytes = tar.extractfile(member).read()\n",
        "      yield member_bytes.decode('utf-8', errors='replace')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkurnJBsf7RX"
      },
      "source": [
        "* 実際にデータを取得しリスト化する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taMYgFMor-j_"
      },
      "source": [
        "docs = list(extract_documents())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPSjQ6PNgBK1"
      },
      "source": [
        "* 文書数、具体的な文書の内容などを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhY0Iq0ycIv4"
      },
      "source": [
        "print(len(docs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSTVJr5jgC5d"
      },
      "source": [
        "print(docs[0][:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM4WRXtQgL9G"
      },
      "source": [
        "### spaCyを使ってtokenizeする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZZGGkqEgQ94"
      },
      "source": [
        "* 前処理の高速化のため、taggerなどは無効にしておく"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi6mRRyjgJq_"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFKq6fvQhNTg"
      },
      "source": [
        "* 小文字にしてからtokenizeする関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LVnUt6FgYbW"
      },
      "source": [
        "def spacy_lemmatize_text(nlp, text):\n",
        "  text = nlp(text.lower())\n",
        "  doc = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text]\n",
        "  return [word for word in doc if len(word) > 1] # 長さ1の単語は削除"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPvZBojxgYcW"
      },
      "source": [
        "* tokenizationの実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iICBt6EgqC-"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "new_docs = list()\n",
        "for doc in tqdm(docs):\n",
        "  new_docs.append(spacy_lemmatize_text(nlp, doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDnVSoZgaMJ"
      },
      "source": [
        "* tokenizationの結果を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MVn-YGhBst"
      },
      "source": [
        "print(new_docs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7lABQFJgdBj"
      },
      "source": [
        "* 各文書を長い一本の文字列で表現\n",
        " * CountVectorizerを後で使うため、トークンを半角スペースでつないだ長い文字列で表しなおす。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnNRbzLzi6vh"
      },
      "source": [
        "corpus = [' '.join(doc) for doc in new_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-72RddHDIME"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nu0eT7OXCzD"
      },
      "source": [
        "## データ行列の作成\n",
        "* NMFの場合、TF-IDFで単語の重みを求めて各文書をベクトル化する。\n",
        "* LDAの場合、単語の出現頻度をそのまま使って各文書をベクトル化する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8oPlaMCzoM"
      },
      "source": [
        "### sklearnのCountVectorizerで疎行列化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pyLBUfSXTUL"
      },
      "source": [
        "* 全文書の半分より多い文書に現れる単語は、高頻度語とみなして削除する。\n",
        "* 10件未満の文書にしか現れない単語は、低頻度語とみなして削除する。\n",
        "* stop wordsも削除する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrj0mmMrCzNI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.5, min_df=10, stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diy077KvDdxi"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LVXuO9GDysO"
      },
      "source": [
        "print(vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoE0bBHJEFEj"
      },
      "source": [
        "print(len(vectorizer.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2wVko9vXnlq"
      },
      "source": [
        "* 文書数と語彙サイズを変数にセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZYC-cLaFV9V"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAd821DGFXXp"
      },
      "source": [
        "n_samples, n_features = X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG8puM9OXrNk"
      },
      "source": [
        "### TF-IDFで各文書における単語の重みを計算する\n",
        "* これはNMFの方だけで使う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywX2HtW-Elar"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer()\n",
        "Xtfidf = tfidf.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdac5_tSE4YC"
      },
      "source": [
        "print(Xtfidf[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0mRZonFEJF"
      },
      "source": [
        "Xtfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdJTl_VYNHV"
      },
      "source": [
        "* 抽出するトピックの個数は、今回は20個とする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjCUJPTsYM5H"
      },
      "source": [
        "n_components = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_CT7nbdUtVX"
      },
      "source": [
        "## NMFでトピック抽出\n",
        "* まず、TF-IDFのデータ行列を使って　NMFによってトピック抽出を試みる。\n",
        " * NMFのパラメータ群は下記サンプルコードのまま。\n",
        " * https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXUdhYDMYuDO"
      },
      "source": [
        "### NMFとLDAのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L34qQ1iFncJ"
      },
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAVXaQM0Y2Tz"
      },
      "source": [
        "### NMFによるトピック抽出の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD5YYXTmFhBj"
      },
      "source": [
        "from time import time\n",
        "\n",
        "print((f\"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "  f\"divergence) with tf-idf features, n_samples={n_samples} \"\n",
        "  f\"and n_features={n_features}\"))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components,\n",
        "          random_state=12345,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, \n",
        "          alpha=.1,\n",
        "          l1_ratio=.5,\n",
        "          verbose=1)\n",
        "nmf.fit(Xtfidf)\n",
        "print(f\"done in {time() - t0:0.3f}s.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* scikit-learn 1.2以降は、下のように書かないと動かなくなるらしい\n",
        " * alphaの代わりにalpha_Wとalpha_Hを使う。\n",
        " * ただし、alphaの設定で使った値を、alpha_Wについては特徴量の個数で、alpha_Hについてはインスタンス数で割らないと、元の使い方と同じ結果にならない。"
      ],
      "metadata": {
        "id": "AyNjnsdoYNr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((f\"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "  f\"divergence) with tf-idf features, n_samples={n_samples} \"\n",
        "  f\"and n_features={n_features}\"))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components,\n",
        "          random_state=12345,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000,\n",
        "          alpha_W=.1 / n_features,\n",
        "          alpha_H=.1 / n_samples,\n",
        "          l1_ratio=.5,\n",
        "          verbose=1)\n",
        "nmf.fit(Xtfidf)\n",
        "print(f\"done in {time() - t0:0.3f}s.\")"
      ],
      "metadata": {
        "id": "7VV7YnrlX4kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg5KZoHYYCN"
      },
      "source": [
        "* NMFにおける各コンポーネントは、それぞれのトピックにおける単語の重要度を表すベクトルとして表現されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve9R8yHfG4AM"
      },
      "source": [
        "nmf.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHsNjupYp7w"
      },
      "source": [
        "### トピックの重要語を取り出す関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpoC-pofHMEO"
      },
      "source": [
        "def get_top_words(model, feature_names, n_top_words=30):\n",
        "  top_features = []\n",
        "  weights = []\n",
        "  for topic_idx, topic in enumerate(model.components_):\n",
        "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
        "    top_features.append([feature_names[i] for i in top_features_ind])\n",
        "    weights.append(topic[top_features_ind])\n",
        "  return top_features, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8K-_A0AZIZ0"
      },
      "source": [
        "### NMFの各コンポーネントから重要語を取り出す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDe-I2RuSvMu"
      },
      "source": [
        "top_words, weights = get_top_words(nmf, vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJsNT2kUULwl"
      },
      "source": [
        "print(top_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN10-rDQU2rt"
      },
      "source": [
        "topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75wn85J2U6gj"
      },
      "source": [
        "topic_words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbQ_eRpZQNs"
      },
      "source": [
        "### 重要語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxUXK24EkgWy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W103IkR3ZWYg"
      },
      "source": [
        "* ワードクラウドから除去するストップワードを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0p-1tK7k7rk"
      },
      "source": [
        "print(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INZcGJAbF_o"
      },
      "source": [
        "* ワードクラウドを描画"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRWtPdcOkwiG"
      },
      "source": [
        "cloud = WordCloud(stopwords=STOPWORDS,\n",
        "                  background_color='white',\n",
        "                  width=1500,\n",
        "                  height=1000,\n",
        "                  max_words=100,\n",
        "                  colormap='tab10'\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmbw1bOHkhfN"
      },
      "source": [
        "fig, axes = plt.subplots(5, 4, figsize=(16, 25), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  cloud.generate_from_frequencies(topic_words[i], max_font_size=300)\n",
        "  plt.gca().imshow(cloud)\n",
        "  plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トピックモデルを使って上のようにワードクラウドを描くと・・・\n",
        " * コーパス全体に対して、下のようにたった一つだけ、ワードクラウドを描くことの大雑把さに気づくかも。"
      ],
      "metadata": {
        "id": "leFe556KcTjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = dict(zip(vectorizer.get_feature_names_out(), Xtfidf.toarray().sum(0)))\n",
        "cloud.generate_from_frequencies(words, max_font_size=300)\n",
        "plt.imshow(cloud);"
      ],
      "metadata": {
        "id": "yxPUOB3lamLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EHfuc6RZgPh"
      },
      "source": [
        "## LDAでトピック抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAEThMwZjhb"
      },
      "source": [
        "### LDAによるトピック抽出の実行\n",
        "* scikit-learnの実装を使う。\n",
        "* `topic_word_prior`と"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuuSTK6fZfu7"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_components, \n",
        "                                max_iter=20,\n",
        "                                doc_topic_prior=0.05,\n",
        "                                topic_word_prior=0.01,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50,\n",
        "                                batch_size=200,\n",
        "                                mean_change_tol=1e-4,\n",
        "                                random_state=12345,\n",
        "                                evaluate_every=1,\n",
        "                                verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 入力データは各文書における各単語の出現回数\n",
        " * TF-IDFのような、出現回数を加工したデータを使うと、LDAというモデルの構成に合わない。"
      ],
      "metadata": {
        "id": "0Zx2JtNtmPmg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWd3DND8aAVz"
      },
      "source": [
        "print((f\"Fitting LDA models with tf features, \"\n",
        "  f\"n_samples={n_samples} and n_features={n_features}\"))\n",
        "t0 = time()\n",
        "lda.fit(X)\n",
        "print(f\"done in {time() - t0:0.3f}s.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pez5Ak2-cUjS"
      },
      "source": [
        "### LDAの各トピックから高確率語を取り出す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBI2nx6aEri"
      },
      "source": [
        "top_words, weights = get_top_words(lda, vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HADZSPDcJbC"
      },
      "source": [
        "print(top_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQgOfcGYb8gA"
      },
      "source": [
        "topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_HRjNZJcbNr"
      },
      "source": [
        "### 高確率語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o3v2CLfb1ku"
      },
      "source": [
        "cloud = WordCloud(stopwords=STOPWORDS,\n",
        "                  background_color='white',\n",
        "                  width=1500,\n",
        "                  height=1000,\n",
        "                  max_words=100,\n",
        "                  colormap='tab10'\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFf5jcX5b45d"
      },
      "source": [
        "fig, axes = plt.subplots(5, 4, figsize=(16, 25), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  cloud.generate_from_frequencies(topic_words[i], max_font_size=300)\n",
        "  plt.gca().imshow(cloud)\n",
        "  plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDAの可視化ツールpyLDAvisを使う"
      ],
      "metadata": {
        "id": "_CgCE_UscGSA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEINkILeopMw"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KGlybhAosnv"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda, X, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZHDUDylpOFM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}