{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Nfp5tfEIi-WEFZ0MFHyz1QM6rI3cxF3-",
      "authorship_tag": "ABX9TyPTF5SNgfSNiUqJpMDigbW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-nlp/blob/main/assignment_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9RRllRm7o6a"
      },
      "source": [
        "# 課題４\n",
        "* 春学期に習った分類手法を使って、IMDbデータセットの感情分析をしてみよう。\n",
        " * training set / test setの分割は、そのまま使う。\n",
        " * training setをどのように使うかはお任せします。（交差検証など。）\n",
        " * test setでの分類性能をArea under the ROC curveで報告。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNQrq8Sg7rp_"
      },
      "source": [
        "## fasttextの単語ベクトルを使う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24JT-d-J6NjL"
      },
      "source": [
        "* 授業で紹介したデータファイルを読み込む\n",
        " * fasttextの単語ベクトルを使ってIMDbデータセットの各文書をベクトル化したデータ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYz8ha5p5AkG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "PATH = '/content/drive/MyDrive/2021Courses/NLP/'\n",
        "\n",
        "texts = dict()\n",
        "labels = dict()\n",
        "for tag in ['train', 'test']:\n",
        "  with open(f'{PATH}{tag}.npy', 'rb') as f:\n",
        "    texts[tag] = np.load(f)\n",
        "  with open(f'{PATH}{tag}_labels.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkq9VlVI5XVm"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "random_state = np.random.RandomState(0)\n",
        "\n",
        "for C in [0.1, 1, 10, 100]:\n",
        "  classifier = LogisticRegression(C=C, random_state=random_state, max_iter=1000)\n",
        "  roc_auc = cross_val_score(classifier, texts['train'], labels['train'], cv=5, scoring='roc_auc').mean()\n",
        "  print(f'C={C} | roc auc : {roc_auc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZHOqHc0AZoq"
      },
      "source": [
        "for C in [200, 500, 1000]:\n",
        "  classifier = LogisticRegression(C=C, random_state=random_state, max_iter=1000)\n",
        "  roc_auc = cross_val_score(classifier, texts['train'], labels['train'], cv=5, scoring='roc_auc').mean()\n",
        "  print(f'C={C} | roc auc : {roc_auc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP5RYE35-NPV"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "classifier = LogisticRegression(C=200, random_state=random_state, max_iter=1000)\n",
        "classifier.fit(texts['train'], labels['train'])\n",
        "print(f\"test roc auc : {roc_auc_score(labels['test'], classifier.predict_proba(texts['test'])[:, 1]):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niDKYtxX7xjh"
      },
      "source": [
        "## TF-IDFで文書をベクトル化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzDBUuft740f"
      },
      "source": [
        "* IMDbデータセットのテキストを取得し直す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE19fVExodCB"
      },
      "source": [
        "!pip install ml_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PwDXpxRofvg"
      },
      "source": [
        "from ml_datasets import imdb\n",
        "train_data, test_data = imdb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHmrEISEojkN"
      },
      "source": [
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uppo7lMYoluS"
      },
      "source": [
        "train_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOybpQFI7-eN"
      },
      "source": [
        "* TF-IDFで文書ベクトルを得る　"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUKNEJxToxTp"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.2)\n",
        "vectorizer.fit(train_texts)\n",
        "X = vectorizer.transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1P6P2ucpREp"
      },
      "source": [
        "print(X.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ-spnUOo2Fo"
      },
      "source": [
        "for C in [0.01, 0.1, 1, 10]:\n",
        "  classifier = LogisticRegression(C=C, random_state=random_state, max_iter=1000)\n",
        "  mean_roc_auc = cross_val_score(classifier, X, train_labels, cv=5, scoring='roc_auc').mean()\n",
        "  print(f'C={C} | roc auc : {roc_auc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd9Gl-tBDPt2"
      },
      "source": [
        "for C in [100, 200]:\n",
        "  classifier = LogisticRegression(C=C, random_state=random_state, max_iter=1000)\n",
        "  mean_roc_auc = cross_val_score(classifier, X, train_labels, cv=5, scoring='roc_auc').mean()\n",
        "  print(f'C={C} | roc auc : {roc_auc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02n8jW8CsT7N"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "classifier = LogisticRegression(random_state=random_state, max_iter=1000)\n",
        "classifier.fit(X, train_labels)\n",
        "print(f\"test roc auc : {roc_auc_score(test_labels, classifier.predict_proba(X_test)[:, 1]):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-hCFybJDrPx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}